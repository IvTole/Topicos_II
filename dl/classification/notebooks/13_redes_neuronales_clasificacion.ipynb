{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <span style=\"color:indigo\">Machine Learning e Inferencia Bayesiana</span> </center> \n",
    "\n",
    "<center>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Centro_Universitario_del_Guadalajara_Logo.png/640px-Centro_Universitario_del_Guadalajara_Logo.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "</center>\n",
    "    \n",
    "<center> <span style=\"color:DarkBlue\">  Tema 13: Redes neuronales, clasificacion </span>  </center>\n",
    "<center> <span style=\"color:Blue\"> M. en C. Iván A. Toledano Juárez </span>  </center>\n",
    "\n",
    "# Clasificación con PyTorch\n",
    "\n",
    "Este notebook está basado en las notas de **MRDBourke** y utiliza datos del famoso dataset del **[Titanic](https://www.kaggle.com/competitions/titanic/overview)** disponible en Kaggle.  \n",
    "El objetivo es construir un modelo de **clasificación binaria** con **PyTorch**, que prediga la probabilidad de supervivencia de los pasajeros.\n",
    "\n",
    "## Objetivo\n",
    "Entrenar una red neuronal simple que aprenda a clasificar a los pasajeros del Titanic según las características disponibles, estimando si **sobrevivieron (1)** o **no sobrevivieron (0)**.\n",
    "\n",
    "## Variables del dataset\n",
    "\n",
    "| Variable | Descripción | Valores posibles |\n",
    "|-----------|--------------|------------------|\n",
    "| `survival` | Supervivencia | 0 = No, 1 = Sí |\n",
    "| `pclass` | Clase del boleto | 1 = 1ª, 2 = 2ª, 3 = 3ª |\n",
    "| `sex` | Sexo | — |\n",
    "| `age` | Edad (en años) | — |\n",
    "| `sibsp` | Nº de hermanos / cónyuges a bordo | — |\n",
    "| `parch` | Nº de padres / hijos a bordo | — |\n",
    "| `ticket` | Número de boleto | — |\n",
    "| `fare` | Tarifa pagada | — |\n",
    "| `cabin` | Número de cabina | — |\n",
    "| `embarked` | Puerto de embarque | C = Cherbourg, Q = Queenstown, S = Southampton |\n",
    "\n",
    "---\n",
    "\n",
    "A lo largo del notebook se realizará el **preprocesamiento de datos**, la **construcción del modelo**, y la **evaluación de su desempeño** mediante métricas de clasificación como **exactitud** y **matriz de confusión**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pclass     Sex   Age  SibSp  Parch     Fare Cabin Embarked  Survived\n",
      "0       3    male  22.0      1      0   7.2500   NaN        S         0\n",
      "1       1  female  38.0      1      0  71.2833   C85        C         1\n",
      "2       3  female  26.0      0      0   7.9250   NaN        S         1\n",
      "3       1  female  35.0      1      0  53.1000  C123        S         1\n",
      "4       3    male  35.0      0      0   8.0500   NaN        S         0\n",
      "Shape(Train) (891, 9)\n",
      "Shape(test) (418, 8)\n"
     ]
    }
   ],
   "source": [
    "df_titanic_train = pd.read_csv(\"titanic_data/train.csv\")\n",
    "df_titanic_test = pd.read_csv(\"titanic_data/test.csv\")\n",
    "\n",
    "# Algunas variables no son necesarias\n",
    "\n",
    "variables = ['Pclass','Sex', 'Age', 'SibSp','Parch', 'Fare', 'Cabin', 'Embarked','Survived']\n",
    "variables_2 = variables.copy()\n",
    "variables_2.remove('Survived')\n",
    "\n",
    "df_titanic_train = df_titanic_train[variables]\n",
    "df_titanic_test = df_titanic_test[variables_2]\n",
    "\n",
    "\n",
    "print(df_titanic_train.head(5))\n",
    "\n",
    "print('Shape(Train)',df_titanic_train.shape)\n",
    "print('Shape(test)',df_titanic_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 9 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Pclass    891 non-null    int64  \n",
      " 1   Sex       891 non-null    object \n",
      " 2   Age       714 non-null    float64\n",
      " 3   SibSp     891 non-null    int64  \n",
      " 4   Parch     891 non-null    int64  \n",
      " 5   Fare      891 non-null    float64\n",
      " 6   Cabin     204 non-null    object \n",
      " 7   Embarked  889 non-null    object \n",
      " 8   Survived  891 non-null    int64  \n",
      "dtypes: float64(2), int64(4), object(3)\n",
      "memory usage: 62.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_titanic_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cabin</th>\n",
       "      <td>687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "Pclass      0\n",
       "Sex         0\n",
       "Age       177\n",
       "SibSp       0\n",
       "Parch       0\n",
       "Fare        0\n",
       "Cabin     687\n",
       "Embarked    2\n",
       "Survived    0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Valores nulos\n",
    "df_titanic_train.isna().sum().to_frame().head(83)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cabin</th>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "Pclass      0\n",
       "Sex         0\n",
       "Age        86\n",
       "SibSp       0\n",
       "Parch       0\n",
       "Fare        1\n",
       "Cabin     327\n",
       "Embarked    0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Valores nulos\n",
    "df_titanic_test.isna().sum().to_frame().head(83)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ignorar la variable Cabin. Para la variable de edad, (... podríamos rellenarlo con la media .... o no)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Pclass','Sex', 'Age', 'SibSp','Parch', 'Fare', 'Embarked','Survived']\n",
    "columns_2 = columns.copy()\n",
    "columns_2.remove('Survived')\n",
    "\n",
    "df2_titanic_train = df_titanic_train.copy()\n",
    "df2_titanic_test = df_titanic_test.copy()\n",
    "\n",
    "df2_titanic_train = df2_titanic_train[columns]\n",
    "df2_titanic_test = df2_titanic_test[columns_2]\n",
    "\n",
    "df2_titanic_train['Age'] = df2_titanic_train['Age'].fillna(df2_titanic_train['Age'].mean())\n",
    "df2_titanic_test['Age'] = df2_titanic_test['Age'].fillna(df2_titanic_test['Age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables categoricas y numéricas\n",
    "\n",
    "categorical = df2_titanic_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical = df2_titanic_train.select_dtypes(include='number').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables dummy con pandas\n",
    "\n",
    "df3_titanic_train = df2_titanic_train.copy()\n",
    "df3_titanic_test = df2_titanic_test.copy()\n",
    "\n",
    "\n",
    "for element in categorical:\n",
    "    tab_dummy = pd.get_dummies(df3_titanic_train[element],prefix=element, dtype=int)\n",
    "    data_new = df3_titanic_train.join(tab_dummy)\n",
    "    df3_titanic_train = data_new\n",
    "    \n",
    "for element in categorical:\n",
    "    tab_dummy = pd.get_dummies(df3_titanic_test[element],prefix=element, dtype=int)\n",
    "    data_new = df3_titanic_test.join(tab_dummy)\n",
    "    df3_titanic_test = data_new\n",
    "\n",
    "# Quitamos las columnas redundantes\n",
    "to_keep = [element for element in df3_titanic_train.columns if element not in categorical]\n",
    "to_keep_2 = [element for element in df3_titanic_test.columns if element not in categorical]\n",
    "\n",
    "df3_titanic_train = df3_titanic_train[to_keep]\n",
    "df3_titanic_test = df3_titanic_test[to_keep_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>2</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>3</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>3</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass        Age  SibSp  Parch     Fare  Survived  Sex_female  Sex_male  \\\n",
       "0         3  22.000000      1      0   7.2500         0           0         1   \n",
       "1         1  38.000000      1      0  71.2833         1           1         0   \n",
       "2         3  26.000000      0      0   7.9250         1           1         0   \n",
       "3         1  35.000000      1      0  53.1000         1           1         0   \n",
       "4         3  35.000000      0      0   8.0500         0           0         1   \n",
       "..      ...        ...    ...    ...      ...       ...         ...       ...   \n",
       "886       2  27.000000      0      0  13.0000         0           0         1   \n",
       "887       1  19.000000      0      0  30.0000         1           1         0   \n",
       "888       3  29.699118      1      2  23.4500         0           1         0   \n",
       "889       1  26.000000      0      0  30.0000         1           0         1   \n",
       "890       3  32.000000      0      0   7.7500         0           0         1   \n",
       "\n",
       "     Embarked_C  Embarked_Q  Embarked_S  \n",
       "0             0           0           1  \n",
       "1             1           0           0  \n",
       "2             0           0           1  \n",
       "3             0           0           1  \n",
       "4             0           0           1  \n",
       "..          ...         ...         ...  \n",
       "886           0           0           1  \n",
       "887           0           0           1  \n",
       "888           0           0           1  \n",
       "889           1           0           0  \n",
       "890           0           1           0  \n",
       "\n",
       "[891 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3_titanic_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features y class\n",
    "X_list = to_keep.copy()\n",
    "X_list.remove('Survived')\n",
    "Y_list = 'Survived'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes a array\n",
    "\n",
    "X_df = df3_titanic_train[X_list]\n",
    "Y_df = df3_titanic_train[Y_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df_train, X_df_test, Y_df_train, Y_df_test = train_test_split(X_df,\n",
    "                                                                Y_df,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array_train = X_df_train.to_numpy()\n",
    "X_array_test = X_df_test.to_numpy()\n",
    "Y_array_train = Y_df_train.to_numpy()\n",
    "Y_array_test = Y_df_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. , 45.5,  0. , ...,  0. ,  0. ,  1. ],\n",
       "       [ 2. , 23. ,  0. , ...,  0. ,  0. ,  1. ],\n",
       "       [ 3. , 32. ,  0. , ...,  0. ,  0. ,  1. ],\n",
       "       ...,\n",
       "       [ 3. , 41. ,  2. , ...,  0. ,  0. ,  1. ],\n",
       "       [ 1. , 14. ,  1. , ...,  0. ,  0. ,  1. ],\n",
       "       [ 1. , 21. ,  0. , ...,  0. ,  0. ,  1. ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura de una red neuronal para clasificación\n",
    "\n",
    "| Hiperparámetro | Clasificación binaria | Clasificación multiclase |\n",
    "| --- | --- | --- |\n",
    "| Capa de entrada | El mismo que el número de variables de entrada | Igual que clasificación binaria|\n",
    "| Capas ocultas | Depende del problema. Teóricamente puede ir de 1 a infinito | Igual que clasificación binaria|\n",
    "| Neuronas por capa oculta | Depende del probleme. Usualmente entre 10 y 512 | Igual que clasificación binaria|\n",
    "| Capas de salida | 1 (una por clase) | Una por cada clase|\n",
    "| Función de activación de capas ocultas | Típica: ReLU, pero puede ser cualquiera. | Igual que clasificación binaria|\n",
    "| Función de activación de capa de salida | Típica: Sigmoid | [Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) |\n",
    "| Loss Function | [Binary crossentropy](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) | [Crossentropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)|\n",
    "| Optimizador | SGD, [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) | Igual que clasificación binaria|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrays a tensores, y sets de entrenamiento y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_array_train).type(torch.float)\n",
    "X_test = torch.from_numpy(X_array_test).type(torch.float)\n",
    "y_train = torch.from_numpy(Y_array_train).type(torch.float)\n",
    "y_test = torch.from_numpy(Y_array_test).type(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando el modelo\n",
    "\n",
    "Ya tenemos nuestros datos listos, así que es momento de **construir un modelo** de clasificación utilizando **PyTorch**.  \n",
    "El proceso lo dividiremos en varios pasos clave:\n",
    "\n",
    "1. Configurar código **agnóstico al dispositivo** (CPU o GPU).  \n",
    "2. Construir un modelo **subclasificando `nn.Module`**.  \n",
    "3. Definir una **función de pérdida** y un **optimizador**.  \n",
    "4. Crear un **bucle de entrenamiento**.\n",
    "\n",
    "La buena noticia es que ya hemos seguido estos pasos antes (en el notebook anterior), solo que ahora los **ajustaremos para un problema de clasificación**.\n",
    "\n",
    "---\n",
    "\n",
    "## Configuración del dispositivo\n",
    "\n",
    "Comenzamos importando las librerías necesarias y preparando el entorno para que el modelo pueda ejecutarse en **CPU o GPU**, según disponibilidad. Si tu equipo tiene acceso a una GPU compatible, pytorch la utilizará automáticamente. Esto permite que todo --datos, modelos y tensores -- se gestionesn en el dispositivo adecuado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fijar el tipo de hardware\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([712, 10])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shapes de tensores\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del modelo\n",
    "\n",
    "Queremos un modelo que reciba nuestros datos de entrada `X`(features) y produzca una predicción `y`, es decir, un tipo de problema supervisado. Para ello, definiremos una clase en python que,\n",
    "\n",
    "* Herede de `nn.module` (como todos los modelos de pytorch)\n",
    "* Cree dos capas lineales `nn.linear` en el constructor, con las dimensiones de entrada y salida adecuadas para nuestros datos.\n",
    "* Implemente un método `forward()` que defina la propagación hacia adelante del modelo\n",
    "* Instanciamos el modelo y lo envíamos al dispositivo configurado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelV0(\n",
       "  (layer_1): Linear(in_features=10, out_features=20, bias=True)\n",
       "  (layer_2): Linear(in_features=20, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Construimos la clase del modelo con la subclase nn.Module\n",
    "class ModelV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 2. Creamos las capas de entrada (lineales) capaces de manejar los features de entrada y clase de salida\n",
    "        self.layer_1 = nn.Linear(in_features=10, out_features=20) # toma 10 features (X), produce 20 features\n",
    "        self.layer_2 = nn.Linear(in_features=20, out_features=1) # toma 20 features, produce 1 feature (y)\n",
    "    \n",
    "    # 3. Definimos un método para la propagación (forward)\n",
    "    def forward(self, x):\n",
    "        # Regresa la capa de salida de layer_2, un solo features, con el mismo shape que y\n",
    "        # El calculo pasa sobre layer_1 y luego su output es el input de layer_2\n",
    "        return self.layer_2(self.layer_1(x)) \n",
    "\n",
    "# 4. Creamos una instancia con el modelo y se manda al hardware\n",
    "model_0 = ModelV0().to(device)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera capa (`layer_1`) recibe 2 características de entrada (`in_features=2`) y produce 5 salidas (`out_features=5`). Estas 5 salidas se conocen como unidades ocultas, y permiten al modelo aprender **patrones más complejos**.\n",
    "\n",
    "La segunda capa (`layer_2`) toma esas 5 características y las transforma en una única salida (`out_features=1`), que corresponden a la predicción del modelo.\n",
    "\n",
    "**NOTA**: El número de unidades ocultas las elige uno. Más unidades podrían capturar patrones más complejos, pero también pueden provocar sobreajuste y entrenamiento más lento.\n",
    "\n",
    "## `nn.Sequential`\n",
    "\n",
    "El método `nn.Sequential()` ejecuta la propagación hacia adelante en el orden en que aparecen las capas, simplificando la sintaxis cuando no se requieren pasos intermedios personalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=10, out_features=20, bias=True)\n",
       "  (1): Linear(in_features=20, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se replica el modelV0\n",
    "model_0 = nn.Sequential(\n",
    "    nn.Linear(in_features=10, out_features=20),\n",
    "    nn.Linear(in_features=20, out_features=1)\n",
    ").to(device)\n",
    "\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of predictions: 179, Shape: torch.Size([179, 1])\n",
      "Length of test samples: 179, Shape: torch.Size([179])\n",
      "\n",
      "First 10 predictions:\n",
      "tensor([[1.3630],\n",
      "        [1.5595],\n",
      "        [1.0445],\n",
      "        [0.4934],\n",
      "        [0.7088],\n",
      "        [1.5590],\n",
      "        [1.5053],\n",
      "        [0.7810],\n",
      "        [0.8900],\n",
      "        [1.0415]], device='mps:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "First 10 test labels:\n",
      "tensor([1., 0., 0., 1., 1., 1., 1., 0., 1., 1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Hacemos predicciones con el modelo\n",
    "untrained_preds = model_0(X_test.to(device))\n",
    "print(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\")\n",
    "print(f\"Length of test samples: {len(y_test)}, Shape: {y_test.shape}\")\n",
    "print(f\"\\nFirst 10 predictions:\\n{untrained_preds[:10]}\")\n",
    "print(f\"\\nFirst 10 test labels:\\n{y_test[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de la función de pérdida y el optimizador\n",
    "\n",
    "Ya hemos configurado modelos, así que ahora toca definir **cómo aprenderá**, a través de una **función de pérdida** (*loss function*) y un **optimizador**.\n",
    "\n",
    "En el notebook previo ya usamos estos conceptos, pero es importante notar que **diferentes tipos de problemas requieren distintas funciones de pérdida.**\n",
    "\n",
    "---\n",
    "\n",
    "## Función de pérdida\n",
    "\n",
    "La función de pérdida (también llamada *cost function*) mide **qué tan equivocadas son las predicciones del modelo**. Mientras más alto sea su valor, peor está aprendiendo el modelo.  El entrenamiento consiste en **minimizar esta pérdida**.\n",
    "\n",
    "Ejemplos comunes:\n",
    "\n",
    "| Función / Optimizador | Tipo de problema | Código en PyTorch |\n",
    "|------------------------|------------------|-------------------|\n",
    "| Stochastic Gradient Descent (SGD) | Clasificación, regresión, muchos otros | `torch.optim.SGD()` |\n",
    "| Adam Optimizer | Clasificación, regresión, muchos otros | `torch.optim.Adam()` |\n",
    "| Binary Cross Entropy (BCE) | Clasificación binaria | `torch.nn.BCELoss()` o `torch.nn.BCEWithLogitsLoss()` |\n",
    "| Cross Entropy | Clasificación multiclase | `torch.nn.CrossEntropyLoss()` |\n",
    "| Mean Absolute Error (MAE) / L1 Loss | Regresión | `torch.nn.L1Loss()` |\n",
    "| Mean Squared Error (MSE) / L2 Loss | Regresión | `torch.nn.MSELoss()` |\n",
    "\n",
    "---\n",
    "\n",
    "## Elección para nuestro caso\n",
    "\n",
    "Como estamos trabajando con un **problema de clasificación binaria**, la opción más adecuada es usar una **pérdida de entropía cruzada binaria** (*binary cross entropy loss*).\n",
    "\n",
    "PyTorch ofrece dos versiones:\n",
    "\n",
    "- `torch.nn.BCELoss()`  \n",
    "  Calcula la entropía cruzada binaria entre las predicciones y las etiquetas.\n",
    "  \n",
    "- `torch.nn.BCEWithLogitsLoss()`  \n",
    "  Hace lo mismo, pero **integra internamente una función sigmoide** (`nn.Sigmoid`).  \n",
    "  Esto la hace **más estable numéricamente** y generalmente se recomienda sobre la anterior.\n",
    "\n",
    "> 💡 **Recomendación:**  \n",
    "> Usa `torch.nn.BCEWithLogitsLoss()` en la mayoría de los casos de clasificación binaria.  \n",
    "> Evita aplicar manualmente un `Sigmoid` si utilizas esta versión.\n",
    "\n",
    "---\n",
    "\n",
    "## Optimizador\n",
    "\n",
    "El optimizador es el algoritmo que **ajusta los pesos del modelo** para minimizar la pérdida.  Podemos usar el clásico **descenso de gradiente estocástico (SGD)** o el más moderno **Adam**. Ambos funcionan bien, pero empezaremos con **SGD** para mayor claridad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid built-in\n",
    "\n",
    "# Optimizador\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), \n",
    "                            lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métrica de evaluación (accuracy)\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calcula si dos tensores son iguales\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo\n",
    "\n",
    "Antes de realizar el loop de entrenamiento, veamos que sale del modelo al realizar una propagación hacia adelante, usando los datos de validacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3630],\n",
       "        [1.5595],\n",
       "        [1.0445],\n",
       "        [0.4934],\n",
       "        [0.7088]], device='mps:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Los 5 primeros outputs\n",
    "y_logits = model_0(X_test.to(device))[:5]\n",
    "y_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como el modelo todavía **no ha sido entrenado**, sus salidas son esencialmente **valores aleatorios**.  Durante la **propagación hacia adelante**, los datos pasan a través de las dos capas lineales definidas, las cuales aplican internamente la siguiente ecuación:\n",
    "\n",
    "\\begin{equation}\n",
    "y = x \\cdot w^T + \\mathrm{bias}\n",
    "\\end{equation}\n",
    "\n",
    "Los valores resultantes $y$ de esta operación, así como los que produce el modelo, se conocen como **_logits_**.  En un modelo puramente lineal, estos logits representarían simplemente **valores numéricos** sin restricción en su rango (pueden ser negativos o positivos).\n",
    "\n",
    "Si aplicamos una **función de activación sigmoide** sobre ellos, podemos convertir dichos valores en **probabilidades** dentro del intervalo $(0, 1)$, lo que nos permite interpretar el resultado como:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{probabilidad de clase positiva} = \\sigma(y) = \\frac{1}{1 + e^{-y}}\n",
    "\\end{equation}\n",
    "\n",
    "De este modo, al establecer un **umbral (threshold)** —por ejemplo, 0.5— podemos transformar las probabilidades en una **clasificación binaria**:\n",
    "\n",
    "- Si $\\sigma(y) \\ge 0.5$ → clase **1 (positivo)**  \n",
    "- Si $\\sigma(y) < 0.5$ → clase **0 (negativo)**\n",
    "\n",
    "> 💡 **Nota:** En PyTorch, cuando se utiliza `nn.BCEWithLogitsLoss`, la función sigmoide ya está incorporada dentro de la función de pérdida, por lo que **no es necesario aplicarla manualmente** en la salida del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7963],\n",
       "        [0.8263],\n",
       "        [0.7397],\n",
       "        [0.6209],\n",
       "        [0.6701]], device='mps:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sigmoid\n",
    "y_pred_probs = torch.sigmoid(y_logits)\n",
    "y_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True], device='mps:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], device='mps:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redondeamos para obtener una clasificación (threshold 0.5)\n",
    "#y_preds = torch.round(y_pred_probs)\n",
    "threshold = 0.5\n",
    "y_preds = (y_pred_probs >= threshold).float()\n",
    "\n",
    "y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))\n",
    "\n",
    "# Checamos igualdad\n",
    "print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))\n",
    "\n",
    "# Quitamos la dimensión extra\n",
    "y_preds.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 1., 1.], device='mps:0')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5]\n",
    "# Vemos que ahora si tenemos las etiquetas que queremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 1.16306, Accuracy: 37.78% | Test loss: 2.00418, Test acc: 58.66%\n",
      "Epoch: 10 | Loss: 1.53208, Accuracy: 43.40% | Test loss: 0.73070, Test acc: 65.36%\n",
      "Epoch: 20 | Loss: 0.77356, Accuracy: 64.47% | Test loss: 0.90248, Test acc: 58.66%\n",
      "Epoch: 30 | Loss: 0.64264, Accuracy: 68.12% | Test loss: 0.64960, Test acc: 62.57%\n",
      "Epoch: 40 | Loss: 1.02167, Accuracy: 61.52% | Test loss: 0.64553, Test acc: 64.80%\n",
      "Epoch: 50 | Loss: 0.71083, Accuracy: 66.85% | Test loss: 0.71363, Test acc: 58.66%\n",
      "Epoch: 60 | Loss: 0.68624, Accuracy: 67.98% | Test loss: 0.67134, Test acc: 59.78%\n",
      "Epoch: 70 | Loss: 0.69462, Accuracy: 68.12% | Test loss: 0.65710, Test acc: 59.22%\n",
      "Epoch: 80 | Loss: 0.67428, Accuracy: 67.98% | Test loss: 0.64648, Test acc: 59.22%\n",
      "Epoch: 90 | Loss: 0.65893, Accuracy: 67.84% | Test loss: 0.63801, Test acc: 59.78%\n",
      "Epoch: 100 | Loss: 0.64717, Accuracy: 68.12% | Test loss: 0.63110, Test acc: 60.89%\n",
      "Epoch: 110 | Loss: 0.63801, Accuracy: 68.54% | Test loss: 0.62537, Test acc: 61.45%\n",
      "Epoch: 120 | Loss: 0.63077, Accuracy: 69.24% | Test loss: 0.62054, Test acc: 62.01%\n",
      "Epoch: 130 | Loss: 0.62496, Accuracy: 70.22% | Test loss: 0.61643, Test acc: 62.01%\n",
      "Epoch: 140 | Loss: 0.62022, Accuracy: 70.65% | Test loss: 0.61290, Test acc: 62.01%\n",
      "Epoch: 150 | Loss: 0.61631, Accuracy: 71.21% | Test loss: 0.60983, Test acc: 62.57%\n",
      "Epoch: 160 | Loss: 0.61303, Accuracy: 71.21% | Test loss: 0.60714, Test acc: 63.13%\n",
      "Epoch: 170 | Loss: 0.61027, Accuracy: 71.49% | Test loss: 0.60477, Test acc: 63.13%\n",
      "Epoch: 180 | Loss: 0.60790, Accuracy: 71.49% | Test loss: 0.60265, Test acc: 63.13%\n",
      "Epoch: 190 | Loss: 0.60586, Accuracy: 71.63% | Test loss: 0.60074, Test acc: 63.69%\n",
      "Epoch: 200 | Loss: 0.60408, Accuracy: 71.63% | Test loss: 0.59901, Test acc: 63.69%\n",
      "Epoch: 210 | Loss: 0.60251, Accuracy: 71.63% | Test loss: 0.59743, Test acc: 64.25%\n",
      "Epoch: 220 | Loss: 0.60111, Accuracy: 71.77% | Test loss: 0.59596, Test acc: 64.25%\n",
      "Epoch: 230 | Loss: 0.59986, Accuracy: 71.77% | Test loss: 0.59460, Test acc: 64.25%\n",
      "Epoch: 240 | Loss: 0.59873, Accuracy: 71.91% | Test loss: 0.59332, Test acc: 64.25%\n",
      "Epoch: 250 | Loss: 0.59769, Accuracy: 71.63% | Test loss: 0.59211, Test acc: 64.25%\n",
      "Epoch: 260 | Loss: 0.59674, Accuracy: 71.63% | Test loss: 0.59096, Test acc: 64.25%\n",
      "Epoch: 270 | Loss: 0.59585, Accuracy: 72.05% | Test loss: 0.58986, Test acc: 64.80%\n",
      "Epoch: 280 | Loss: 0.59503, Accuracy: 71.77% | Test loss: 0.58879, Test acc: 64.80%\n",
      "Epoch: 290 | Loss: 0.59425, Accuracy: 71.63% | Test loss: 0.58777, Test acc: 64.80%\n",
      "Epoch: 300 | Loss: 0.59352, Accuracy: 71.77% | Test loss: 0.58677, Test acc: 64.80%\n",
      "Epoch: 310 | Loss: 0.59281, Accuracy: 71.63% | Test loss: 0.58580, Test acc: 65.92%\n",
      "Epoch: 320 | Loss: 0.59214, Accuracy: 71.63% | Test loss: 0.58486, Test acc: 65.92%\n",
      "Epoch: 330 | Loss: 0.59150, Accuracy: 71.63% | Test loss: 0.58393, Test acc: 65.92%\n",
      "Epoch: 340 | Loss: 0.59088, Accuracy: 71.63% | Test loss: 0.58302, Test acc: 65.92%\n",
      "Epoch: 350 | Loss: 0.59027, Accuracy: 71.63% | Test loss: 0.58213, Test acc: 65.92%\n",
      "Epoch: 360 | Loss: 0.58969, Accuracy: 71.77% | Test loss: 0.58125, Test acc: 65.92%\n",
      "Epoch: 370 | Loss: 0.58912, Accuracy: 71.63% | Test loss: 0.58039, Test acc: 65.92%\n",
      "Epoch: 380 | Loss: 0.58856, Accuracy: 71.63% | Test loss: 0.57954, Test acc: 65.92%\n",
      "Epoch: 390 | Loss: 0.58802, Accuracy: 71.91% | Test loss: 0.57870, Test acc: 65.92%\n",
      "Epoch: 400 | Loss: 0.58748, Accuracy: 71.77% | Test loss: 0.57787, Test acc: 65.92%\n",
      "Epoch: 410 | Loss: 0.58696, Accuracy: 71.91% | Test loss: 0.57705, Test acc: 67.04%\n",
      "Epoch: 420 | Loss: 0.58644, Accuracy: 72.05% | Test loss: 0.57624, Test acc: 67.04%\n",
      "Epoch: 430 | Loss: 0.58593, Accuracy: 72.05% | Test loss: 0.57545, Test acc: 67.60%\n",
      "Epoch: 440 | Loss: 0.58543, Accuracy: 72.33% | Test loss: 0.57466, Test acc: 67.60%\n",
      "Epoch: 450 | Loss: 0.58494, Accuracy: 72.61% | Test loss: 0.57388, Test acc: 68.16%\n",
      "Epoch: 460 | Loss: 0.58445, Accuracy: 72.75% | Test loss: 0.57310, Test acc: 68.16%\n",
      "Epoch: 470 | Loss: 0.58397, Accuracy: 72.61% | Test loss: 0.57234, Test acc: 68.16%\n",
      "Epoch: 480 | Loss: 0.58350, Accuracy: 72.61% | Test loss: 0.57159, Test acc: 68.16%\n",
      "Epoch: 490 | Loss: 0.58303, Accuracy: 72.61% | Test loss: 0.57084, Test acc: 68.16%\n",
      "Epoch: 500 | Loss: 0.58256, Accuracy: 72.75% | Test loss: 0.57010, Test acc: 68.16%\n",
      "Epoch: 510 | Loss: 0.58210, Accuracy: 72.89% | Test loss: 0.56937, Test acc: 67.60%\n",
      "Epoch: 520 | Loss: 0.58165, Accuracy: 73.46% | Test loss: 0.56865, Test acc: 67.60%\n",
      "Epoch: 530 | Loss: 0.58120, Accuracy: 73.60% | Test loss: 0.56793, Test acc: 67.60%\n",
      "Epoch: 540 | Loss: 0.58075, Accuracy: 73.60% | Test loss: 0.56723, Test acc: 67.60%\n",
      "Epoch: 550 | Loss: 0.58031, Accuracy: 73.88% | Test loss: 0.56653, Test acc: 67.60%\n",
      "Epoch: 560 | Loss: 0.57987, Accuracy: 73.88% | Test loss: 0.56583, Test acc: 67.60%\n",
      "Epoch: 570 | Loss: 0.57943, Accuracy: 73.88% | Test loss: 0.56515, Test acc: 67.60%\n",
      "Epoch: 580 | Loss: 0.57900, Accuracy: 74.16% | Test loss: 0.56447, Test acc: 67.60%\n",
      "Epoch: 590 | Loss: 0.57857, Accuracy: 74.58% | Test loss: 0.56380, Test acc: 67.60%\n",
      "Epoch: 600 | Loss: 0.57814, Accuracy: 74.72% | Test loss: 0.56313, Test acc: 67.60%\n",
      "Epoch: 610 | Loss: 0.57772, Accuracy: 74.72% | Test loss: 0.56247, Test acc: 67.60%\n",
      "Epoch: 620 | Loss: 0.57730, Accuracy: 75.00% | Test loss: 0.56182, Test acc: 67.60%\n",
      "Epoch: 630 | Loss: 0.57688, Accuracy: 74.86% | Test loss: 0.56118, Test acc: 67.60%\n",
      "Epoch: 640 | Loss: 0.57647, Accuracy: 74.86% | Test loss: 0.56054, Test acc: 67.60%\n",
      "Epoch: 650 | Loss: 0.57605, Accuracy: 74.72% | Test loss: 0.55991, Test acc: 67.60%\n",
      "Epoch: 660 | Loss: 0.57564, Accuracy: 74.86% | Test loss: 0.55928, Test acc: 67.60%\n",
      "Epoch: 670 | Loss: 0.57523, Accuracy: 74.86% | Test loss: 0.55866, Test acc: 67.60%\n",
      "Epoch: 680 | Loss: 0.57483, Accuracy: 74.86% | Test loss: 0.55805, Test acc: 67.60%\n",
      "Epoch: 690 | Loss: 0.57442, Accuracy: 75.14% | Test loss: 0.55744, Test acc: 67.60%\n",
      "Epoch: 700 | Loss: 0.57402, Accuracy: 75.28% | Test loss: 0.55684, Test acc: 67.60%\n",
      "Epoch: 710 | Loss: 0.57362, Accuracy: 75.28% | Test loss: 0.55624, Test acc: 68.16%\n",
      "Epoch: 720 | Loss: 0.57323, Accuracy: 75.42% | Test loss: 0.55565, Test acc: 68.16%\n",
      "Epoch: 730 | Loss: 0.57283, Accuracy: 75.42% | Test loss: 0.55507, Test acc: 68.16%\n",
      "Epoch: 740 | Loss: 0.57244, Accuracy: 75.56% | Test loss: 0.55449, Test acc: 68.16%\n",
      "Epoch: 750 | Loss: 0.57204, Accuracy: 75.56% | Test loss: 0.55392, Test acc: 68.16%\n",
      "Epoch: 760 | Loss: 0.57165, Accuracy: 75.56% | Test loss: 0.55335, Test acc: 68.16%\n",
      "Epoch: 770 | Loss: 0.57127, Accuracy: 75.84% | Test loss: 0.55279, Test acc: 68.16%\n",
      "Epoch: 780 | Loss: 0.57088, Accuracy: 75.84% | Test loss: 0.55223, Test acc: 68.16%\n",
      "Epoch: 790 | Loss: 0.57050, Accuracy: 75.84% | Test loss: 0.55168, Test acc: 68.16%\n",
      "Epoch: 800 | Loss: 0.57011, Accuracy: 75.98% | Test loss: 0.55113, Test acc: 68.16%\n",
      "Epoch: 810 | Loss: 0.56973, Accuracy: 76.12% | Test loss: 0.55059, Test acc: 68.16%\n",
      "Epoch: 820 | Loss: 0.56935, Accuracy: 75.84% | Test loss: 0.55005, Test acc: 68.16%\n",
      "Epoch: 830 | Loss: 0.56897, Accuracy: 75.70% | Test loss: 0.54952, Test acc: 68.72%\n",
      "Epoch: 840 | Loss: 0.56860, Accuracy: 75.42% | Test loss: 0.54899, Test acc: 68.72%\n",
      "Epoch: 850 | Loss: 0.56822, Accuracy: 75.42% | Test loss: 0.54847, Test acc: 68.72%\n",
      "Epoch: 860 | Loss: 0.56785, Accuracy: 75.56% | Test loss: 0.54795, Test acc: 68.72%\n",
      "Epoch: 870 | Loss: 0.56748, Accuracy: 75.42% | Test loss: 0.54743, Test acc: 68.72%\n",
      "Epoch: 880 | Loss: 0.56711, Accuracy: 75.56% | Test loss: 0.54692, Test acc: 68.72%\n",
      "Epoch: 890 | Loss: 0.56674, Accuracy: 75.56% | Test loss: 0.54642, Test acc: 68.72%\n",
      "Epoch: 900 | Loss: 0.56638, Accuracy: 75.56% | Test loss: 0.54592, Test acc: 68.72%\n",
      "Epoch: 910 | Loss: 0.56601, Accuracy: 75.56% | Test loss: 0.54542, Test acc: 68.72%\n",
      "Epoch: 920 | Loss: 0.56565, Accuracy: 75.56% | Test loss: 0.54493, Test acc: 68.72%\n",
      "Epoch: 930 | Loss: 0.56529, Accuracy: 75.84% | Test loss: 0.54444, Test acc: 68.72%\n",
      "Epoch: 940 | Loss: 0.56493, Accuracy: 75.84% | Test loss: 0.54396, Test acc: 68.72%\n",
      "Epoch: 950 | Loss: 0.56457, Accuracy: 75.84% | Test loss: 0.54348, Test acc: 68.72%\n",
      "Epoch: 960 | Loss: 0.56421, Accuracy: 75.98% | Test loss: 0.54300, Test acc: 69.27%\n",
      "Epoch: 970 | Loss: 0.56386, Accuracy: 75.98% | Test loss: 0.54253, Test acc: 69.27%\n",
      "Epoch: 980 | Loss: 0.56350, Accuracy: 76.40% | Test loss: 0.54206, Test acc: 69.27%\n",
      "Epoch: 990 | Loss: 0.56315, Accuracy: 77.11% | Test loss: 0.54159, Test acc: 69.27%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(88) # semilla aleatoria\n",
    "\n",
    "# Número de epocas\n",
    "epochs = 1000\n",
    "\n",
    "# Poner los datos en el hardware target\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "# Loop de training y eval\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_0.train()\n",
    "\n",
    "    # 1. Forward propagation (el modelo regresa logits)\n",
    "    y_logits = model_0(X_train).squeeze() # squeeze para remover `1` dimension extra\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> pred probs -> pred labls\n",
    "  \n",
    "    # 2. Se calcula loss/accuracy\n",
    "    # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n",
    "    #                y_train) \n",
    "    loss = loss_fn(y_logits, # nn.BCEWithLogitsLoss acepta los logits de salida\n",
    "                   y_train)\n",
    "    acc = accuracy_fn(y_true=y_train, \n",
    "                      y_pred=y_pred) \n",
    "\n",
    "    # 3. Zero grad para el optimizador\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Back propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizador\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Evaluacion\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward \n",
    "        test_logits = model_0(X_test).squeeze() \n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        # 2. loss/accuracy\n",
    "        test_loss = loss_fn(test_logits,\n",
    "                            y_test)\n",
    "        \n",
    "        test_acc = accuracy_fn(y_true=y_test,\n",
    "                               y_pred=test_pred)\n",
    "\n",
    "    # Print cada 10 epocas\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n",
    "        train_loss_list.append(loss)\n",
    "        test_loss_list.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.163063  , 1.5320771 , 0.7735554 , 0.6426399 , 1.0216701 ,\n",
       "       0.7108254 , 0.6862402 , 0.69462234, 0.67428136, 0.65892565,\n",
       "       0.6471669 , 0.638014  , 0.6307739 , 0.624958  , 0.62021875,\n",
       "       0.6163052 , 0.6130331 , 0.61026603, 0.60790056, 0.6058575 ,\n",
       "       0.60407543, 0.6025059 , 0.6011107 , 0.5998588 , 0.59872586,\n",
       "       0.5976911 , 0.5967386 , 0.5958549 , 0.59502894, 0.59425163,\n",
       "       0.5935157 , 0.5928149 , 0.59214425, 0.5914993 , 0.59087706,\n",
       "       0.59027416, 0.58968836, 0.58911777, 0.5885606 , 0.5880154 ,\n",
       "       0.5874809 , 0.58695614, 0.5864405 , 0.58593297, 0.585433  ,\n",
       "       0.58493984, 0.5844534 , 0.58397293, 0.58349806, 0.58302855,\n",
       "       0.58256406, 0.58210427, 0.58164907, 0.58119804, 0.5807511 ,\n",
       "       0.580308  , 0.57986844, 0.57943255, 0.5789999 , 0.5785703 ,\n",
       "       0.5781439 , 0.5777204 , 0.5772996 , 0.5768816 , 0.57646614,\n",
       "       0.5760531 , 0.57564247, 0.5752344 , 0.57482845, 0.57442456,\n",
       "       0.5740229 , 0.5736235 , 0.5732258 , 0.5728303 , 0.57243645,\n",
       "       0.5720448 , 0.57165474, 0.5712667 , 0.5708804 , 0.5704959 ,\n",
       "       0.5701131 , 0.56973207, 0.5693526 , 0.568975  , 0.5685989 ,\n",
       "       0.56822455, 0.5678519 , 0.56748056, 0.5671111 , 0.56674325,\n",
       "       0.5663769 , 0.5660121 , 0.56564885, 0.5652873 , 0.5649272 ,\n",
       "       0.56456876, 0.5642117 , 0.5638562 , 0.5635024 , 0.5631497 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss_list_cpu = torch.Tensor(train_loss_list).cpu().detach().numpy()\n",
    "train_loss_list_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.0041788 , 0.7307008 , 0.9024806 , 0.6495987 , 0.64553213,\n",
       "       0.7136336 , 0.67134345, 0.6571004 , 0.6464777 , 0.63801265,\n",
       "       0.63110256, 0.62536746, 0.6205414 , 0.6164325 , 0.6128982 ,\n",
       "       0.6098295 , 0.60714114, 0.6047655 , 0.6026471 , 0.60074157,\n",
       "       0.5990119 , 0.5974278 , 0.5959644 , 0.5946011 , 0.59332126,\n",
       "       0.59211075, 0.59095854, 0.5898555 , 0.5887939 , 0.58776766,\n",
       "       0.5867718 , 0.58580226, 0.58485574, 0.58392936, 0.58302116,\n",
       "       0.5821292 , 0.5812519 , 0.58038825, 0.57953703, 0.5786977 ,\n",
       "       0.57786924, 0.5770513 , 0.5762434 , 0.57544506, 0.574656  ,\n",
       "       0.57387596, 0.57310456, 0.5723418 , 0.57158726, 0.57084095,\n",
       "       0.5701026 , 0.5693722 , 0.5686494 , 0.56793433, 0.56722665,\n",
       "       0.56652635, 0.56583333, 0.5651475 , 0.56446886, 0.563797  ,\n",
       "       0.56313205, 0.56247395, 0.5618224 , 0.56117743, 0.5605391 ,\n",
       "       0.5599071 , 0.55928123, 0.55866176, 0.55804837, 0.5574408 ,\n",
       "       0.5568393 , 0.5562436 , 0.5556537 , 0.5550694 , 0.5544906 ,\n",
       "       0.5539174 , 0.55334955, 0.5527869 , 0.55222964, 0.5516774 ,\n",
       "       0.55113024, 0.5505881 , 0.5500506 , 0.5495182 , 0.5489904 ,\n",
       "       0.5484672 , 0.54794866, 0.5474345 , 0.54692495, 0.5464196 ,\n",
       "       0.5459185 , 0.5454217 , 0.54492897, 0.5444404 , 0.5439558 ,\n",
       "       0.54347503, 0.5429983 , 0.54252523, 0.542056  , 0.5415904 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss_list_cpu = torch.Tensor(test_loss_list).cpu().detach().numpy()\n",
    "test_loss_list_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_array = np.arange(0,100,1)\n",
    "plt.plot(x_array, train_loss_list_cpu, color='blue', label=\"train loss\")\n",
    "plt.plot(x_array, test_loss_list_cpu, color='orange', label=\"test loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejorando el modelo\n",
    "\n",
    "Una vez que el modelo básico está funcionando, existen diversas estrategias para **mejorar su desempeño**.  Cada una de las siguientes técnicas busca aumentar la capacidad del modelo para **aprender patrones más complejos** o **ajustarse mejor a los datos**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Añadir más capas\n",
    "Cada capa adicional puede incrementar la **capacidad de representación** del modelo, permitiéndole aprender **patrones más abstractos y no lineales**. Agregar más capas hace que la red sea más **profunda**, lo que da origen al término *deep learning*.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Añadir más neuronas ocultas\n",
    "De forma similar, aumentar el número de **neuronas (unidades ocultas)** dentro de una capa puede mejorar la capacidad del modelo para capturar relaciones complejas entre las variables. Sin embargo, demasiadas neuronas pueden llevar al **sobreajuste (overfitting)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Entrenar por más épocas\n",
    "Dar al modelo más **épocas** (iteraciones completas sobre los datos) permite que los pesos se actualicen más veces, lo que puede mejorar el rendimiento si el modelo aún no ha convergido. Pero un número excesivo de épocas también puede causar **sobreajuste**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Cambiar la función de activación\n",
    "Los datos reales rara vez son lineales. Usar funciones de activación **no lineales** (como ReLU, tanh o sigmoid) permite que el modelo aprenda relaciones más complejas.  \n",
    "Por ejemplo:\n",
    "- `nn.ReLU()` → común en redes profundas.  \n",
    "- `nn.Sigmoid()` → útil en clasificación binaria.  \n",
    "- `nn.Tanh()` → centrada en 0, útil para ciertos tipos de datos.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Ajustar la tasa de aprendizaje\n",
    "La **tasa de aprendizaje** (`learning_rate`) controla qué tanto se ajustan los parámetros en cada actualización.  \n",
    "- Si es **demasiado alta**, el modelo puede **oscilar o divergir**.  \n",
    "- Si es **demasiado baja**, el aprendizaje será **muy lento** o se quedará estancado en un mínimo local.  \n",
    "\n",
    "Encontrar un valor adecuado requiere **experimentación o búsqueda sistemática (grid/random search)**.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Cambiar la función de pérdida\n",
    "Cada tipo de problema (clasificación binaria, multiclase, regresión, etc.) requiere una **función de pérdida diferente**.  \n",
    "Probar distintas opciones puede mejorar la estabilidad o precisión del aprendizaje.\n",
    "\n",
    "Ejemplo:\n",
    "- Clasificación binaria → `nn.BCEWithLogitsLoss()`\n",
    "- Clasificación multiclase → `nn.CrossEntropyLoss()`\n",
    "- Regresión → `nn.MSELoss()` o `nn.L1Loss()`\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Transfer learning\n",
    "En lugar de entrenar un modelo desde cero, se puede **aprovechar un modelo preentrenado** en un problema similar y **ajustarlo (fine-tuning)** a los nuevos datos.  \n",
    "Esta técnica es muy útil cuando se dispone de **pocos datos** o se trabaja con **dominios complejos**, como imágenes o texto.\n",
    "\n",
    "---\n",
    "\n",
    "> **NOTA:** No existe una receta única para mejorar el modelo.  \n",
    "> La práctica más común es **ajustar un hiperparámetro a la vez**, observar su impacto en la pérdida y en las métricas de validación, y repetir el proceso hasta encontrar un equilibrio entre **precisión y generalización**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelV1(\n",
       "  (layer_1): Linear(in_features=10, out_features=20, bias=True)\n",
       "  (layer_2): Linear(in_features=20, out_features=20, bias=True)\n",
       "  (layer_3): Linear(in_features=20, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=10, out_features=20)\n",
    "        self.layer_2 = nn.Linear(in_features=20, out_features=20) # capa extra\n",
    "        self.layer_3 = nn.Linear(in_features=20, out_features=1)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        # z = self.layer_1(x)\n",
    "        # z = self.layer_2(z)\n",
    "        # z = self.layer_3(z)\n",
    "        # return z\n",
    "        return self.layer_3(self.layer_2(self.layer_1(x)))\n",
    "\n",
    "model_1 = ModelV1().to(device)\n",
    "model_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss() \n",
    "optimizer = torch.optim.SGD(model_1.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.48872, Accuracy: 80.06% | Test loss: 0.47162, Test acc: 77.09%\n",
      "Epoch: 10 | Loss: 0.48860, Accuracy: 80.06% | Test loss: 0.47149, Test acc: 77.09%\n",
      "Epoch: 20 | Loss: 0.48848, Accuracy: 80.06% | Test loss: 0.47137, Test acc: 77.09%\n",
      "Epoch: 30 | Loss: 0.48836, Accuracy: 80.06% | Test loss: 0.47124, Test acc: 76.54%\n",
      "Epoch: 40 | Loss: 0.48825, Accuracy: 80.06% | Test loss: 0.47111, Test acc: 76.54%\n",
      "Epoch: 50 | Loss: 0.48813, Accuracy: 80.06% | Test loss: 0.47098, Test acc: 76.54%\n",
      "Epoch: 60 | Loss: 0.48801, Accuracy: 80.06% | Test loss: 0.47086, Test acc: 76.54%\n",
      "Epoch: 70 | Loss: 0.48789, Accuracy: 80.06% | Test loss: 0.47073, Test acc: 76.54%\n",
      "Epoch: 80 | Loss: 0.48778, Accuracy: 80.06% | Test loss: 0.47061, Test acc: 76.54%\n",
      "Epoch: 90 | Loss: 0.48766, Accuracy: 80.06% | Test loss: 0.47048, Test acc: 77.09%\n",
      "Epoch: 100 | Loss: 0.48755, Accuracy: 80.06% | Test loss: 0.47036, Test acc: 77.09%\n",
      "Epoch: 110 | Loss: 0.48743, Accuracy: 80.20% | Test loss: 0.47023, Test acc: 77.09%\n",
      "Epoch: 120 | Loss: 0.48732, Accuracy: 80.20% | Test loss: 0.47011, Test acc: 77.09%\n",
      "Epoch: 130 | Loss: 0.48720, Accuracy: 80.20% | Test loss: 0.46999, Test acc: 77.09%\n",
      "Epoch: 140 | Loss: 0.48709, Accuracy: 80.20% | Test loss: 0.46987, Test acc: 77.09%\n",
      "Epoch: 150 | Loss: 0.48698, Accuracy: 80.20% | Test loss: 0.46974, Test acc: 77.09%\n",
      "Epoch: 160 | Loss: 0.48687, Accuracy: 79.92% | Test loss: 0.46962, Test acc: 77.09%\n",
      "Epoch: 170 | Loss: 0.48676, Accuracy: 79.92% | Test loss: 0.46950, Test acc: 77.09%\n",
      "Epoch: 180 | Loss: 0.48664, Accuracy: 79.78% | Test loss: 0.46938, Test acc: 77.09%\n",
      "Epoch: 190 | Loss: 0.48653, Accuracy: 79.78% | Test loss: 0.46926, Test acc: 77.09%\n",
      "Epoch: 200 | Loss: 0.48642, Accuracy: 79.78% | Test loss: 0.46914, Test acc: 77.09%\n",
      "Epoch: 210 | Loss: 0.48631, Accuracy: 79.92% | Test loss: 0.46902, Test acc: 77.09%\n",
      "Epoch: 220 | Loss: 0.48621, Accuracy: 79.92% | Test loss: 0.46891, Test acc: 77.09%\n",
      "Epoch: 230 | Loss: 0.48610, Accuracy: 79.92% | Test loss: 0.46879, Test acc: 77.09%\n",
      "Epoch: 240 | Loss: 0.48599, Accuracy: 79.92% | Test loss: 0.46867, Test acc: 77.09%\n",
      "Epoch: 250 | Loss: 0.48588, Accuracy: 79.92% | Test loss: 0.46855, Test acc: 77.09%\n",
      "Epoch: 260 | Loss: 0.48578, Accuracy: 79.78% | Test loss: 0.46844, Test acc: 77.09%\n",
      "Epoch: 270 | Loss: 0.48567, Accuracy: 79.78% | Test loss: 0.46832, Test acc: 77.09%\n",
      "Epoch: 280 | Loss: 0.48556, Accuracy: 79.78% | Test loss: 0.46820, Test acc: 77.09%\n",
      "Epoch: 290 | Loss: 0.48546, Accuracy: 79.78% | Test loss: 0.46809, Test acc: 77.09%\n",
      "Epoch: 300 | Loss: 0.48535, Accuracy: 79.78% | Test loss: 0.46797, Test acc: 77.09%\n",
      "Epoch: 310 | Loss: 0.48525, Accuracy: 79.92% | Test loss: 0.46786, Test acc: 77.09%\n",
      "Epoch: 320 | Loss: 0.48514, Accuracy: 79.92% | Test loss: 0.46775, Test acc: 77.09%\n",
      "Epoch: 330 | Loss: 0.48504, Accuracy: 79.92% | Test loss: 0.46763, Test acc: 77.09%\n",
      "Epoch: 340 | Loss: 0.48494, Accuracy: 79.92% | Test loss: 0.46752, Test acc: 77.09%\n",
      "Epoch: 350 | Loss: 0.48484, Accuracy: 79.92% | Test loss: 0.46741, Test acc: 77.09%\n",
      "Epoch: 360 | Loss: 0.48473, Accuracy: 80.06% | Test loss: 0.46730, Test acc: 77.09%\n",
      "Epoch: 370 | Loss: 0.48463, Accuracy: 80.06% | Test loss: 0.46718, Test acc: 77.09%\n",
      "Epoch: 380 | Loss: 0.48453, Accuracy: 80.06% | Test loss: 0.46707, Test acc: 77.09%\n",
      "Epoch: 390 | Loss: 0.48443, Accuracy: 80.34% | Test loss: 0.46696, Test acc: 77.09%\n",
      "Epoch: 400 | Loss: 0.48433, Accuracy: 80.34% | Test loss: 0.46685, Test acc: 77.09%\n",
      "Epoch: 410 | Loss: 0.48423, Accuracy: 80.62% | Test loss: 0.46674, Test acc: 77.09%\n",
      "Epoch: 420 | Loss: 0.48413, Accuracy: 80.62% | Test loss: 0.46663, Test acc: 77.09%\n",
      "Epoch: 430 | Loss: 0.48403, Accuracy: 80.62% | Test loss: 0.46652, Test acc: 77.09%\n",
      "Epoch: 440 | Loss: 0.48393, Accuracy: 80.62% | Test loss: 0.46641, Test acc: 77.09%\n",
      "Epoch: 450 | Loss: 0.48384, Accuracy: 80.62% | Test loss: 0.46631, Test acc: 77.09%\n",
      "Epoch: 460 | Loss: 0.48374, Accuracy: 80.62% | Test loss: 0.46620, Test acc: 77.09%\n",
      "Epoch: 470 | Loss: 0.48364, Accuracy: 80.62% | Test loss: 0.46609, Test acc: 77.09%\n",
      "Epoch: 480 | Loss: 0.48354, Accuracy: 80.62% | Test loss: 0.46598, Test acc: 77.09%\n",
      "Epoch: 490 | Loss: 0.48345, Accuracy: 80.62% | Test loss: 0.46588, Test acc: 77.09%\n",
      "Epoch: 500 | Loss: 0.48335, Accuracy: 80.62% | Test loss: 0.46577, Test acc: 77.09%\n",
      "Epoch: 510 | Loss: 0.48326, Accuracy: 80.62% | Test loss: 0.46566, Test acc: 77.09%\n",
      "Epoch: 520 | Loss: 0.48316, Accuracy: 80.62% | Test loss: 0.46556, Test acc: 77.09%\n",
      "Epoch: 530 | Loss: 0.48307, Accuracy: 80.62% | Test loss: 0.46545, Test acc: 77.09%\n",
      "Epoch: 540 | Loss: 0.48297, Accuracy: 80.62% | Test loss: 0.46535, Test acc: 77.09%\n",
      "Epoch: 550 | Loss: 0.48288, Accuracy: 80.62% | Test loss: 0.46525, Test acc: 77.09%\n",
      "Epoch: 560 | Loss: 0.48279, Accuracy: 80.62% | Test loss: 0.46514, Test acc: 77.09%\n",
      "Epoch: 570 | Loss: 0.48269, Accuracy: 80.76% | Test loss: 0.46504, Test acc: 77.09%\n",
      "Epoch: 580 | Loss: 0.48260, Accuracy: 80.76% | Test loss: 0.46494, Test acc: 77.09%\n",
      "Epoch: 590 | Loss: 0.48251, Accuracy: 80.76% | Test loss: 0.46483, Test acc: 77.09%\n",
      "Epoch: 600 | Loss: 0.48242, Accuracy: 80.76% | Test loss: 0.46473, Test acc: 77.09%\n",
      "Epoch: 610 | Loss: 0.48233, Accuracy: 80.76% | Test loss: 0.46463, Test acc: 77.09%\n",
      "Epoch: 620 | Loss: 0.48223, Accuracy: 80.76% | Test loss: 0.46453, Test acc: 77.09%\n",
      "Epoch: 630 | Loss: 0.48214, Accuracy: 80.76% | Test loss: 0.46443, Test acc: 77.09%\n",
      "Epoch: 640 | Loss: 0.48205, Accuracy: 80.76% | Test loss: 0.46433, Test acc: 77.09%\n",
      "Epoch: 650 | Loss: 0.48196, Accuracy: 80.76% | Test loss: 0.46423, Test acc: 77.09%\n",
      "Epoch: 660 | Loss: 0.48188, Accuracy: 80.62% | Test loss: 0.46413, Test acc: 77.65%\n",
      "Epoch: 670 | Loss: 0.48179, Accuracy: 80.62% | Test loss: 0.46403, Test acc: 77.65%\n",
      "Epoch: 680 | Loss: 0.48170, Accuracy: 80.62% | Test loss: 0.46393, Test acc: 77.65%\n",
      "Epoch: 690 | Loss: 0.48161, Accuracy: 80.62% | Test loss: 0.46383, Test acc: 77.65%\n",
      "Epoch: 700 | Loss: 0.48152, Accuracy: 80.62% | Test loss: 0.46373, Test acc: 77.65%\n",
      "Epoch: 710 | Loss: 0.48144, Accuracy: 80.62% | Test loss: 0.46363, Test acc: 77.65%\n",
      "Epoch: 720 | Loss: 0.48135, Accuracy: 80.62% | Test loss: 0.46353, Test acc: 77.65%\n",
      "Epoch: 730 | Loss: 0.48126, Accuracy: 80.62% | Test loss: 0.46344, Test acc: 77.65%\n",
      "Epoch: 740 | Loss: 0.48118, Accuracy: 80.62% | Test loss: 0.46334, Test acc: 77.65%\n",
      "Epoch: 750 | Loss: 0.48109, Accuracy: 80.62% | Test loss: 0.46324, Test acc: 77.65%\n",
      "Epoch: 760 | Loss: 0.48100, Accuracy: 80.62% | Test loss: 0.46315, Test acc: 77.65%\n",
      "Epoch: 770 | Loss: 0.48092, Accuracy: 80.62% | Test loss: 0.46305, Test acc: 77.65%\n",
      "Epoch: 780 | Loss: 0.48083, Accuracy: 80.62% | Test loss: 0.46296, Test acc: 77.65%\n",
      "Epoch: 790 | Loss: 0.48075, Accuracy: 80.62% | Test loss: 0.46286, Test acc: 77.65%\n",
      "Epoch: 800 | Loss: 0.48067, Accuracy: 80.62% | Test loss: 0.46277, Test acc: 77.65%\n",
      "Epoch: 810 | Loss: 0.48058, Accuracy: 80.76% | Test loss: 0.46267, Test acc: 77.65%\n",
      "Epoch: 820 | Loss: 0.48050, Accuracy: 80.62% | Test loss: 0.46258, Test acc: 77.65%\n",
      "Epoch: 830 | Loss: 0.48042, Accuracy: 80.62% | Test loss: 0.46249, Test acc: 77.65%\n",
      "Epoch: 840 | Loss: 0.48033, Accuracy: 81.04% | Test loss: 0.46239, Test acc: 77.65%\n",
      "Epoch: 850 | Loss: 0.48025, Accuracy: 81.04% | Test loss: 0.46230, Test acc: 77.65%\n",
      "Epoch: 860 | Loss: 0.48017, Accuracy: 81.04% | Test loss: 0.46221, Test acc: 77.65%\n",
      "Epoch: 870 | Loss: 0.48009, Accuracy: 81.04% | Test loss: 0.46211, Test acc: 77.65%\n",
      "Epoch: 880 | Loss: 0.48001, Accuracy: 81.04% | Test loss: 0.46202, Test acc: 77.65%\n",
      "Epoch: 890 | Loss: 0.47993, Accuracy: 81.18% | Test loss: 0.46193, Test acc: 77.65%\n",
      "Epoch: 900 | Loss: 0.47985, Accuracy: 81.18% | Test loss: 0.46184, Test acc: 77.65%\n",
      "Epoch: 910 | Loss: 0.47977, Accuracy: 81.18% | Test loss: 0.46175, Test acc: 77.65%\n",
      "Epoch: 920 | Loss: 0.47969, Accuracy: 81.18% | Test loss: 0.46166, Test acc: 77.65%\n",
      "Epoch: 930 | Loss: 0.47961, Accuracy: 81.18% | Test loss: 0.46157, Test acc: 77.65%\n",
      "Epoch: 940 | Loss: 0.47953, Accuracy: 81.18% | Test loss: 0.46148, Test acc: 77.65%\n",
      "Epoch: 950 | Loss: 0.47945, Accuracy: 81.18% | Test loss: 0.46139, Test acc: 77.65%\n",
      "Epoch: 960 | Loss: 0.47937, Accuracy: 81.18% | Test loss: 0.46130, Test acc: 77.65%\n",
      "Epoch: 970 | Loss: 0.47929, Accuracy: 81.18% | Test loss: 0.46121, Test acc: 77.65%\n",
      "Epoch: 980 | Loss: 0.47922, Accuracy: 81.18% | Test loss: 0.46112, Test acc: 77.65%\n",
      "Epoch: 990 | Loss: 0.47914, Accuracy: 81.18% | Test loss: 0.46104, Test acc: 77.65%\n",
      "Epoch: 1000 | Loss: 0.47906, Accuracy: 81.18% | Test loss: 0.46095, Test acc: 77.65%\n",
      "Epoch: 1010 | Loss: 0.47898, Accuracy: 81.18% | Test loss: 0.46086, Test acc: 77.65%\n",
      "Epoch: 1020 | Loss: 0.47891, Accuracy: 81.18% | Test loss: 0.46077, Test acc: 77.65%\n",
      "Epoch: 1030 | Loss: 0.47883, Accuracy: 81.18% | Test loss: 0.46069, Test acc: 77.65%\n",
      "Epoch: 1040 | Loss: 0.47876, Accuracy: 81.18% | Test loss: 0.46060, Test acc: 77.65%\n",
      "Epoch: 1050 | Loss: 0.47868, Accuracy: 81.18% | Test loss: 0.46051, Test acc: 77.65%\n",
      "Epoch: 1060 | Loss: 0.47861, Accuracy: 81.18% | Test loss: 0.46043, Test acc: 77.65%\n",
      "Epoch: 1070 | Loss: 0.47853, Accuracy: 81.18% | Test loss: 0.46034, Test acc: 77.65%\n",
      "Epoch: 1080 | Loss: 0.47846, Accuracy: 81.18% | Test loss: 0.46026, Test acc: 77.65%\n",
      "Epoch: 1090 | Loss: 0.47838, Accuracy: 81.18% | Test loss: 0.46017, Test acc: 77.65%\n",
      "Epoch: 1100 | Loss: 0.47831, Accuracy: 81.18% | Test loss: 0.46009, Test acc: 77.65%\n",
      "Epoch: 1110 | Loss: 0.47823, Accuracy: 81.32% | Test loss: 0.46000, Test acc: 77.65%\n",
      "Epoch: 1120 | Loss: 0.47816, Accuracy: 81.32% | Test loss: 0.45992, Test acc: 77.65%\n",
      "Epoch: 1130 | Loss: 0.47809, Accuracy: 81.32% | Test loss: 0.45984, Test acc: 77.65%\n",
      "Epoch: 1140 | Loss: 0.47802, Accuracy: 81.32% | Test loss: 0.45975, Test acc: 77.65%\n",
      "Epoch: 1150 | Loss: 0.47794, Accuracy: 81.32% | Test loss: 0.45967, Test acc: 77.65%\n",
      "Epoch: 1160 | Loss: 0.47787, Accuracy: 81.32% | Test loss: 0.45959, Test acc: 78.21%\n",
      "Epoch: 1170 | Loss: 0.47780, Accuracy: 81.32% | Test loss: 0.45950, Test acc: 78.21%\n",
      "Epoch: 1180 | Loss: 0.47773, Accuracy: 81.32% | Test loss: 0.45942, Test acc: 78.21%\n",
      "Epoch: 1190 | Loss: 0.47766, Accuracy: 81.32% | Test loss: 0.45934, Test acc: 78.21%\n",
      "Epoch: 1200 | Loss: 0.47759, Accuracy: 81.18% | Test loss: 0.45926, Test acc: 78.21%\n",
      "Epoch: 1210 | Loss: 0.47752, Accuracy: 81.32% | Test loss: 0.45918, Test acc: 78.21%\n",
      "Epoch: 1220 | Loss: 0.47745, Accuracy: 81.32% | Test loss: 0.45910, Test acc: 78.21%\n",
      "Epoch: 1230 | Loss: 0.47738, Accuracy: 81.32% | Test loss: 0.45902, Test acc: 78.21%\n",
      "Epoch: 1240 | Loss: 0.47731, Accuracy: 81.32% | Test loss: 0.45894, Test acc: 78.21%\n",
      "Epoch: 1250 | Loss: 0.47724, Accuracy: 81.32% | Test loss: 0.45886, Test acc: 78.21%\n",
      "Epoch: 1260 | Loss: 0.47717, Accuracy: 81.32% | Test loss: 0.45878, Test acc: 78.21%\n",
      "Epoch: 1270 | Loss: 0.47710, Accuracy: 81.32% | Test loss: 0.45870, Test acc: 78.21%\n",
      "Epoch: 1280 | Loss: 0.47703, Accuracy: 81.32% | Test loss: 0.45862, Test acc: 78.21%\n",
      "Epoch: 1290 | Loss: 0.47696, Accuracy: 81.32% | Test loss: 0.45854, Test acc: 78.21%\n",
      "Epoch: 1300 | Loss: 0.47689, Accuracy: 81.32% | Test loss: 0.45846, Test acc: 78.21%\n",
      "Epoch: 1310 | Loss: 0.47683, Accuracy: 81.32% | Test loss: 0.45838, Test acc: 78.21%\n",
      "Epoch: 1320 | Loss: 0.47676, Accuracy: 81.32% | Test loss: 0.45830, Test acc: 78.21%\n",
      "Epoch: 1330 | Loss: 0.47669, Accuracy: 81.32% | Test loss: 0.45822, Test acc: 78.21%\n",
      "Epoch: 1340 | Loss: 0.47662, Accuracy: 81.32% | Test loss: 0.45815, Test acc: 78.21%\n",
      "Epoch: 1350 | Loss: 0.47656, Accuracy: 81.32% | Test loss: 0.45807, Test acc: 78.21%\n",
      "Epoch: 1360 | Loss: 0.47649, Accuracy: 81.32% | Test loss: 0.45799, Test acc: 78.21%\n",
      "Epoch: 1370 | Loss: 0.47642, Accuracy: 81.32% | Test loss: 0.45792, Test acc: 78.21%\n",
      "Epoch: 1380 | Loss: 0.47636, Accuracy: 81.32% | Test loss: 0.45784, Test acc: 78.21%\n",
      "Epoch: 1390 | Loss: 0.47629, Accuracy: 81.32% | Test loss: 0.45776, Test acc: 78.21%\n",
      "Epoch: 1400 | Loss: 0.47623, Accuracy: 81.32% | Test loss: 0.45769, Test acc: 78.21%\n",
      "Epoch: 1410 | Loss: 0.47616, Accuracy: 81.32% | Test loss: 0.45761, Test acc: 78.21%\n",
      "Epoch: 1420 | Loss: 0.47610, Accuracy: 81.32% | Test loss: 0.45754, Test acc: 78.21%\n",
      "Epoch: 1430 | Loss: 0.47603, Accuracy: 81.32% | Test loss: 0.45746, Test acc: 78.21%\n",
      "Epoch: 1440 | Loss: 0.47597, Accuracy: 81.32% | Test loss: 0.45739, Test acc: 78.21%\n",
      "Epoch: 1450 | Loss: 0.47590, Accuracy: 81.32% | Test loss: 0.45731, Test acc: 78.21%\n",
      "Epoch: 1460 | Loss: 0.47584, Accuracy: 81.32% | Test loss: 0.45724, Test acc: 78.21%\n",
      "Epoch: 1470 | Loss: 0.47578, Accuracy: 81.32% | Test loss: 0.45716, Test acc: 78.21%\n",
      "Epoch: 1480 | Loss: 0.47571, Accuracy: 81.32% | Test loss: 0.45709, Test acc: 78.21%\n",
      "Epoch: 1490 | Loss: 0.47565, Accuracy: 81.32% | Test loss: 0.45702, Test acc: 78.21%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(88) # semilla aleatoria\n",
    "\n",
    "# Número de epocas\n",
    "epochs = 1500\n",
    "\n",
    "# Poner los datos en el hardware target\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Loop de training y eval\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_1.train()\n",
    "\n",
    "    # 1. Forward propagation (el modelo regresa logits)\n",
    "    y_logits = model_1(X_train).squeeze() # squeeze para remover `1` dimension extra\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> pred probs -> pred labls\n",
    "  \n",
    "    # 2. Se calcula loss/accuracy\n",
    "    # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n",
    "    #                y_train) \n",
    "    loss = loss_fn(y_logits, # nn.BCEWithLogitsLoss acepta los logits de salida\n",
    "                   y_train) \n",
    "    acc = accuracy_fn(y_true=y_train, \n",
    "                      y_pred=y_pred) \n",
    "\n",
    "    # 3. Zero grad para el optimizador\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Back propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizador\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Evaluacion\n",
    "    model_1.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward \n",
    "        test_logits = model_1(X_test).squeeze() \n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        # 2. loss/accuracy\n",
    "        test_loss = loss_fn(test_logits,\n",
    "                            y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test,\n",
    "                               y_pred=test_pred)\n",
    "\n",
    "    # Print cada 10 epocas\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelV2(\n",
      "  (layer_1): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (layer_2): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (layer_3): Linear(in_features=20, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ModelV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=10, out_features=20)\n",
    "        self.layer_2 = nn.Linear(in_features=20, out_features=20)\n",
    "        self.layer_3 = nn.Linear(in_features=20, out_features=1)\n",
    "        self.relu = nn.ReLU() # <- Se añade función de activación ReLU\n",
    "        # También se puede usar sigmoid, pero se tendría que quitar en la parte de transformación del output \n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "      # ReLU se aplica entre capas\n",
    "       return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n",
    "\n",
    "model_3 = ModelV2().to(device)\n",
    "print(model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.47559, Accuracy: 81.32% | Test loss: 0.45694, Test acc: 78.21%\n",
      "Epoch: 10 | Loss: 0.47553, Accuracy: 81.32% | Test loss: 0.45687, Test acc: 78.21%\n",
      "Epoch: 20 | Loss: 0.47546, Accuracy: 81.32% | Test loss: 0.45680, Test acc: 78.21%\n",
      "Epoch: 30 | Loss: 0.47540, Accuracy: 81.32% | Test loss: 0.45673, Test acc: 78.21%\n",
      "Epoch: 40 | Loss: 0.47534, Accuracy: 81.32% | Test loss: 0.45665, Test acc: 78.21%\n",
      "Epoch: 50 | Loss: 0.47528, Accuracy: 81.32% | Test loss: 0.45658, Test acc: 78.21%\n",
      "Epoch: 60 | Loss: 0.47522, Accuracy: 81.32% | Test loss: 0.45651, Test acc: 78.21%\n",
      "Epoch: 70 | Loss: 0.47516, Accuracy: 81.32% | Test loss: 0.45644, Test acc: 78.21%\n",
      "Epoch: 80 | Loss: 0.47509, Accuracy: 81.32% | Test loss: 0.45637, Test acc: 78.21%\n",
      "Epoch: 90 | Loss: 0.47503, Accuracy: 81.32% | Test loss: 0.45630, Test acc: 78.21%\n",
      "Epoch: 100 | Loss: 0.47497, Accuracy: 81.32% | Test loss: 0.45623, Test acc: 78.21%\n",
      "Epoch: 110 | Loss: 0.47491, Accuracy: 81.32% | Test loss: 0.45616, Test acc: 78.21%\n",
      "Epoch: 120 | Loss: 0.47485, Accuracy: 81.18% | Test loss: 0.45609, Test acc: 77.65%\n",
      "Epoch: 130 | Loss: 0.47479, Accuracy: 81.18% | Test loss: 0.45602, Test acc: 77.65%\n",
      "Epoch: 140 | Loss: 0.47473, Accuracy: 81.18% | Test loss: 0.45595, Test acc: 77.65%\n",
      "Epoch: 150 | Loss: 0.47468, Accuracy: 81.04% | Test loss: 0.45588, Test acc: 77.65%\n",
      "Epoch: 160 | Loss: 0.47462, Accuracy: 81.04% | Test loss: 0.45581, Test acc: 77.65%\n",
      "Epoch: 170 | Loss: 0.47456, Accuracy: 81.04% | Test loss: 0.45574, Test acc: 77.65%\n",
      "Epoch: 180 | Loss: 0.47450, Accuracy: 81.04% | Test loss: 0.45567, Test acc: 77.65%\n",
      "Epoch: 190 | Loss: 0.47444, Accuracy: 81.04% | Test loss: 0.45560, Test acc: 77.65%\n",
      "Epoch: 200 | Loss: 0.47438, Accuracy: 81.04% | Test loss: 0.45553, Test acc: 77.65%\n",
      "Epoch: 210 | Loss: 0.47432, Accuracy: 81.04% | Test loss: 0.45547, Test acc: 77.65%\n",
      "Epoch: 220 | Loss: 0.47427, Accuracy: 81.04% | Test loss: 0.45540, Test acc: 77.65%\n",
      "Epoch: 230 | Loss: 0.47421, Accuracy: 81.04% | Test loss: 0.45533, Test acc: 77.65%\n",
      "Epoch: 240 | Loss: 0.47415, Accuracy: 81.04% | Test loss: 0.45526, Test acc: 77.65%\n",
      "Epoch: 250 | Loss: 0.47410, Accuracy: 81.04% | Test loss: 0.45520, Test acc: 77.65%\n",
      "Epoch: 260 | Loss: 0.47404, Accuracy: 81.04% | Test loss: 0.45513, Test acc: 77.65%\n",
      "Epoch: 270 | Loss: 0.47398, Accuracy: 81.04% | Test loss: 0.45506, Test acc: 77.65%\n",
      "Epoch: 280 | Loss: 0.47393, Accuracy: 81.04% | Test loss: 0.45500, Test acc: 77.65%\n",
      "Epoch: 290 | Loss: 0.47387, Accuracy: 81.04% | Test loss: 0.45493, Test acc: 77.65%\n",
      "Epoch: 300 | Loss: 0.47381, Accuracy: 81.04% | Test loss: 0.45486, Test acc: 77.65%\n",
      "Epoch: 310 | Loss: 0.47376, Accuracy: 81.04% | Test loss: 0.45480, Test acc: 77.65%\n",
      "Epoch: 320 | Loss: 0.47370, Accuracy: 81.04% | Test loss: 0.45473, Test acc: 77.65%\n",
      "Epoch: 330 | Loss: 0.47365, Accuracy: 81.04% | Test loss: 0.45467, Test acc: 77.65%\n",
      "Epoch: 340 | Loss: 0.47359, Accuracy: 81.04% | Test loss: 0.45460, Test acc: 77.65%\n",
      "Epoch: 350 | Loss: 0.47354, Accuracy: 81.04% | Test loss: 0.45454, Test acc: 77.65%\n",
      "Epoch: 360 | Loss: 0.47348, Accuracy: 81.04% | Test loss: 0.45447, Test acc: 77.65%\n",
      "Epoch: 370 | Loss: 0.47343, Accuracy: 81.04% | Test loss: 0.45441, Test acc: 77.65%\n",
      "Epoch: 380 | Loss: 0.47337, Accuracy: 81.04% | Test loss: 0.45434, Test acc: 77.65%\n",
      "Epoch: 390 | Loss: 0.47332, Accuracy: 81.04% | Test loss: 0.45428, Test acc: 77.65%\n",
      "Epoch: 400 | Loss: 0.47327, Accuracy: 81.04% | Test loss: 0.45422, Test acc: 77.65%\n",
      "Epoch: 410 | Loss: 0.47321, Accuracy: 81.04% | Test loss: 0.45415, Test acc: 77.65%\n",
      "Epoch: 420 | Loss: 0.47316, Accuracy: 81.18% | Test loss: 0.45409, Test acc: 77.65%\n",
      "Epoch: 430 | Loss: 0.47311, Accuracy: 81.18% | Test loss: 0.45403, Test acc: 77.65%\n",
      "Epoch: 440 | Loss: 0.47305, Accuracy: 81.18% | Test loss: 0.45396, Test acc: 77.65%\n",
      "Epoch: 450 | Loss: 0.47300, Accuracy: 81.18% | Test loss: 0.45390, Test acc: 77.65%\n",
      "Epoch: 460 | Loss: 0.47295, Accuracy: 81.18% | Test loss: 0.45384, Test acc: 77.65%\n",
      "Epoch: 470 | Loss: 0.47289, Accuracy: 81.18% | Test loss: 0.45377, Test acc: 77.65%\n",
      "Epoch: 480 | Loss: 0.47284, Accuracy: 81.18% | Test loss: 0.45371, Test acc: 77.65%\n",
      "Epoch: 490 | Loss: 0.47279, Accuracy: 81.18% | Test loss: 0.45365, Test acc: 77.65%\n",
      "Epoch: 500 | Loss: 0.47274, Accuracy: 81.18% | Test loss: 0.45359, Test acc: 77.65%\n",
      "Epoch: 510 | Loss: 0.47269, Accuracy: 81.18% | Test loss: 0.45353, Test acc: 77.65%\n",
      "Epoch: 520 | Loss: 0.47264, Accuracy: 81.18% | Test loss: 0.45347, Test acc: 77.65%\n",
      "Epoch: 530 | Loss: 0.47258, Accuracy: 81.18% | Test loss: 0.45341, Test acc: 77.65%\n",
      "Epoch: 540 | Loss: 0.47253, Accuracy: 81.18% | Test loss: 0.45334, Test acc: 77.65%\n",
      "Epoch: 550 | Loss: 0.47248, Accuracy: 81.18% | Test loss: 0.45328, Test acc: 77.65%\n",
      "Epoch: 560 | Loss: 0.47243, Accuracy: 81.18% | Test loss: 0.45322, Test acc: 77.65%\n",
      "Epoch: 570 | Loss: 0.47238, Accuracy: 81.18% | Test loss: 0.45316, Test acc: 77.65%\n",
      "Epoch: 580 | Loss: 0.47233, Accuracy: 81.18% | Test loss: 0.45310, Test acc: 77.65%\n",
      "Epoch: 590 | Loss: 0.47228, Accuracy: 81.18% | Test loss: 0.45304, Test acc: 77.65%\n",
      "Epoch: 600 | Loss: 0.47223, Accuracy: 81.18% | Test loss: 0.45298, Test acc: 77.65%\n",
      "Epoch: 610 | Loss: 0.47218, Accuracy: 81.18% | Test loss: 0.45292, Test acc: 77.65%\n",
      "Epoch: 620 | Loss: 0.47213, Accuracy: 81.18% | Test loss: 0.45286, Test acc: 77.65%\n",
      "Epoch: 630 | Loss: 0.47208, Accuracy: 81.18% | Test loss: 0.45281, Test acc: 77.65%\n",
      "Epoch: 640 | Loss: 0.47203, Accuracy: 81.18% | Test loss: 0.45275, Test acc: 77.65%\n",
      "Epoch: 650 | Loss: 0.47198, Accuracy: 81.18% | Test loss: 0.45269, Test acc: 77.65%\n",
      "Epoch: 660 | Loss: 0.47193, Accuracy: 81.18% | Test loss: 0.45263, Test acc: 77.65%\n",
      "Epoch: 670 | Loss: 0.47189, Accuracy: 81.18% | Test loss: 0.45257, Test acc: 77.65%\n",
      "Epoch: 680 | Loss: 0.47184, Accuracy: 81.18% | Test loss: 0.45251, Test acc: 77.65%\n",
      "Epoch: 690 | Loss: 0.47179, Accuracy: 81.18% | Test loss: 0.45245, Test acc: 77.65%\n",
      "Epoch: 700 | Loss: 0.47174, Accuracy: 81.18% | Test loss: 0.45240, Test acc: 77.65%\n",
      "Epoch: 710 | Loss: 0.47169, Accuracy: 81.18% | Test loss: 0.45234, Test acc: 77.65%\n",
      "Epoch: 720 | Loss: 0.47164, Accuracy: 81.18% | Test loss: 0.45228, Test acc: 77.65%\n",
      "Epoch: 730 | Loss: 0.47160, Accuracy: 81.18% | Test loss: 0.45222, Test acc: 77.65%\n",
      "Epoch: 740 | Loss: 0.47155, Accuracy: 81.18% | Test loss: 0.45217, Test acc: 77.65%\n",
      "Epoch: 750 | Loss: 0.47150, Accuracy: 81.18% | Test loss: 0.45211, Test acc: 77.65%\n",
      "Epoch: 760 | Loss: 0.47145, Accuracy: 81.18% | Test loss: 0.45205, Test acc: 77.65%\n",
      "Epoch: 770 | Loss: 0.47141, Accuracy: 81.18% | Test loss: 0.45200, Test acc: 77.65%\n",
      "Epoch: 780 | Loss: 0.47136, Accuracy: 81.18% | Test loss: 0.45194, Test acc: 77.65%\n",
      "Epoch: 790 | Loss: 0.47131, Accuracy: 81.18% | Test loss: 0.45188, Test acc: 77.65%\n",
      "Epoch: 800 | Loss: 0.47127, Accuracy: 81.18% | Test loss: 0.45183, Test acc: 77.65%\n",
      "Epoch: 810 | Loss: 0.47122, Accuracy: 81.18% | Test loss: 0.45177, Test acc: 77.65%\n",
      "Epoch: 820 | Loss: 0.47117, Accuracy: 81.18% | Test loss: 0.45172, Test acc: 77.65%\n",
      "Epoch: 830 | Loss: 0.47113, Accuracy: 81.18% | Test loss: 0.45166, Test acc: 77.65%\n",
      "Epoch: 840 | Loss: 0.47108, Accuracy: 81.18% | Test loss: 0.45161, Test acc: 77.65%\n",
      "Epoch: 850 | Loss: 0.47104, Accuracy: 81.18% | Test loss: 0.45155, Test acc: 77.65%\n",
      "Epoch: 860 | Loss: 0.47099, Accuracy: 81.18% | Test loss: 0.45150, Test acc: 77.65%\n",
      "Epoch: 870 | Loss: 0.47095, Accuracy: 81.18% | Test loss: 0.45144, Test acc: 77.65%\n",
      "Epoch: 880 | Loss: 0.47090, Accuracy: 81.18% | Test loss: 0.45139, Test acc: 77.65%\n",
      "Epoch: 890 | Loss: 0.47086, Accuracy: 81.18% | Test loss: 0.45133, Test acc: 77.65%\n",
      "Epoch: 900 | Loss: 0.47081, Accuracy: 81.18% | Test loss: 0.45128, Test acc: 77.65%\n",
      "Epoch: 910 | Loss: 0.47077, Accuracy: 81.18% | Test loss: 0.45122, Test acc: 77.65%\n",
      "Epoch: 920 | Loss: 0.47072, Accuracy: 81.18% | Test loss: 0.45117, Test acc: 77.65%\n",
      "Epoch: 930 | Loss: 0.47068, Accuracy: 81.18% | Test loss: 0.45112, Test acc: 77.65%\n",
      "Epoch: 940 | Loss: 0.47063, Accuracy: 81.18% | Test loss: 0.45106, Test acc: 77.65%\n",
      "Epoch: 950 | Loss: 0.47059, Accuracy: 81.18% | Test loss: 0.45101, Test acc: 77.65%\n",
      "Epoch: 960 | Loss: 0.47054, Accuracy: 81.18% | Test loss: 0.45096, Test acc: 77.65%\n",
      "Epoch: 970 | Loss: 0.47050, Accuracy: 81.18% | Test loss: 0.45090, Test acc: 77.65%\n",
      "Epoch: 980 | Loss: 0.47046, Accuracy: 81.18% | Test loss: 0.45085, Test acc: 77.65%\n",
      "Epoch: 990 | Loss: 0.47041, Accuracy: 81.18% | Test loss: 0.45080, Test acc: 78.21%\n",
      "Epoch: 1000 | Loss: 0.47037, Accuracy: 81.18% | Test loss: 0.45074, Test acc: 78.21%\n",
      "Epoch: 1010 | Loss: 0.47033, Accuracy: 81.18% | Test loss: 0.45069, Test acc: 78.21%\n",
      "Epoch: 1020 | Loss: 0.47028, Accuracy: 81.18% | Test loss: 0.45064, Test acc: 78.21%\n",
      "Epoch: 1030 | Loss: 0.47024, Accuracy: 81.18% | Test loss: 0.45059, Test acc: 78.21%\n",
      "Epoch: 1040 | Loss: 0.47020, Accuracy: 81.18% | Test loss: 0.45053, Test acc: 78.21%\n",
      "Epoch: 1050 | Loss: 0.47016, Accuracy: 81.18% | Test loss: 0.45048, Test acc: 78.21%\n",
      "Epoch: 1060 | Loss: 0.47011, Accuracy: 81.32% | Test loss: 0.45043, Test acc: 78.21%\n",
      "Epoch: 1070 | Loss: 0.47007, Accuracy: 81.32% | Test loss: 0.45038, Test acc: 78.21%\n",
      "Epoch: 1080 | Loss: 0.47003, Accuracy: 81.32% | Test loss: 0.45033, Test acc: 78.21%\n",
      "Epoch: 1090 | Loss: 0.46999, Accuracy: 81.18% | Test loss: 0.45028, Test acc: 78.21%\n",
      "Epoch: 1100 | Loss: 0.46994, Accuracy: 81.18% | Test loss: 0.45023, Test acc: 78.21%\n",
      "Epoch: 1110 | Loss: 0.46990, Accuracy: 81.18% | Test loss: 0.45017, Test acc: 78.21%\n",
      "Epoch: 1120 | Loss: 0.46986, Accuracy: 81.18% | Test loss: 0.45012, Test acc: 78.21%\n",
      "Epoch: 1130 | Loss: 0.46982, Accuracy: 81.32% | Test loss: 0.45007, Test acc: 78.21%\n",
      "Epoch: 1140 | Loss: 0.46978, Accuracy: 81.32% | Test loss: 0.45002, Test acc: 78.21%\n",
      "Epoch: 1150 | Loss: 0.46974, Accuracy: 81.32% | Test loss: 0.44997, Test acc: 78.21%\n",
      "Epoch: 1160 | Loss: 0.46970, Accuracy: 81.60% | Test loss: 0.44992, Test acc: 78.21%\n",
      "Epoch: 1170 | Loss: 0.46965, Accuracy: 81.60% | Test loss: 0.44987, Test acc: 78.21%\n",
      "Epoch: 1180 | Loss: 0.46961, Accuracy: 81.60% | Test loss: 0.44982, Test acc: 78.21%\n",
      "Epoch: 1190 | Loss: 0.46957, Accuracy: 81.60% | Test loss: 0.44977, Test acc: 78.21%\n",
      "Epoch: 1200 | Loss: 0.46953, Accuracy: 81.60% | Test loss: 0.44972, Test acc: 78.21%\n",
      "Epoch: 1210 | Loss: 0.46949, Accuracy: 81.60% | Test loss: 0.44967, Test acc: 78.21%\n",
      "Epoch: 1220 | Loss: 0.46945, Accuracy: 81.60% | Test loss: 0.44962, Test acc: 78.21%\n",
      "Epoch: 1230 | Loss: 0.46941, Accuracy: 81.60% | Test loss: 0.44957, Test acc: 78.21%\n",
      "Epoch: 1240 | Loss: 0.46937, Accuracy: 81.60% | Test loss: 0.44953, Test acc: 78.21%\n",
      "Epoch: 1250 | Loss: 0.46933, Accuracy: 81.60% | Test loss: 0.44948, Test acc: 78.21%\n",
      "Epoch: 1260 | Loss: 0.46929, Accuracy: 81.60% | Test loss: 0.44943, Test acc: 78.21%\n",
      "Epoch: 1270 | Loss: 0.46925, Accuracy: 81.60% | Test loss: 0.44938, Test acc: 78.21%\n",
      "Epoch: 1280 | Loss: 0.46921, Accuracy: 81.60% | Test loss: 0.44933, Test acc: 78.21%\n",
      "Epoch: 1290 | Loss: 0.46917, Accuracy: 81.60% | Test loss: 0.44928, Test acc: 78.21%\n",
      "Epoch: 1300 | Loss: 0.46913, Accuracy: 81.60% | Test loss: 0.44923, Test acc: 78.21%\n",
      "Epoch: 1310 | Loss: 0.46909, Accuracy: 81.60% | Test loss: 0.44919, Test acc: 78.21%\n",
      "Epoch: 1320 | Loss: 0.46906, Accuracy: 81.60% | Test loss: 0.44914, Test acc: 78.21%\n",
      "Epoch: 1330 | Loss: 0.46902, Accuracy: 81.60% | Test loss: 0.44909, Test acc: 78.21%\n",
      "Epoch: 1340 | Loss: 0.46898, Accuracy: 81.60% | Test loss: 0.44904, Test acc: 78.21%\n",
      "Epoch: 1350 | Loss: 0.46894, Accuracy: 81.60% | Test loss: 0.44900, Test acc: 78.21%\n",
      "Epoch: 1360 | Loss: 0.46890, Accuracy: 81.60% | Test loss: 0.44895, Test acc: 78.21%\n",
      "Epoch: 1370 | Loss: 0.46886, Accuracy: 81.60% | Test loss: 0.44890, Test acc: 78.21%\n",
      "Epoch: 1380 | Loss: 0.46882, Accuracy: 81.60% | Test loss: 0.44885, Test acc: 78.21%\n",
      "Epoch: 1390 | Loss: 0.46879, Accuracy: 81.60% | Test loss: 0.44881, Test acc: 78.21%\n",
      "Epoch: 1400 | Loss: 0.46875, Accuracy: 81.60% | Test loss: 0.44876, Test acc: 78.21%\n",
      "Epoch: 1410 | Loss: 0.46871, Accuracy: 81.60% | Test loss: 0.44871, Test acc: 78.21%\n",
      "Epoch: 1420 | Loss: 0.46867, Accuracy: 81.60% | Test loss: 0.44867, Test acc: 78.21%\n",
      "Epoch: 1430 | Loss: 0.46864, Accuracy: 81.60% | Test loss: 0.44862, Test acc: 78.21%\n",
      "Epoch: 1440 | Loss: 0.46860, Accuracy: 81.60% | Test loss: 0.44857, Test acc: 78.21%\n",
      "Epoch: 1450 | Loss: 0.46856, Accuracy: 81.60% | Test loss: 0.44853, Test acc: 78.21%\n",
      "Epoch: 1460 | Loss: 0.46852, Accuracy: 81.60% | Test loss: 0.44848, Test acc: 78.21%\n",
      "Epoch: 1470 | Loss: 0.46849, Accuracy: 81.60% | Test loss: 0.44844, Test acc: 78.21%\n",
      "Epoch: 1480 | Loss: 0.46845, Accuracy: 81.60% | Test loss: 0.44839, Test acc: 78.21%\n",
      "Epoch: 1490 | Loss: 0.46841, Accuracy: 81.60% | Test loss: 0.44834, Test acc: 78.21%\n",
      "Epoch: 1500 | Loss: 0.46838, Accuracy: 81.60% | Test loss: 0.44830, Test acc: 78.21%\n",
      "Epoch: 1510 | Loss: 0.46834, Accuracy: 81.60% | Test loss: 0.44825, Test acc: 78.21%\n",
      "Epoch: 1520 | Loss: 0.46830, Accuracy: 81.60% | Test loss: 0.44821, Test acc: 78.21%\n",
      "Epoch: 1530 | Loss: 0.46827, Accuracy: 81.60% | Test loss: 0.44816, Test acc: 78.21%\n",
      "Epoch: 1540 | Loss: 0.46823, Accuracy: 81.60% | Test loss: 0.44812, Test acc: 78.21%\n",
      "Epoch: 1550 | Loss: 0.46819, Accuracy: 81.60% | Test loss: 0.44807, Test acc: 78.21%\n",
      "Epoch: 1560 | Loss: 0.46816, Accuracy: 81.46% | Test loss: 0.44803, Test acc: 78.21%\n",
      "Epoch: 1570 | Loss: 0.46812, Accuracy: 81.46% | Test loss: 0.44798, Test acc: 78.21%\n",
      "Epoch: 1580 | Loss: 0.46809, Accuracy: 81.46% | Test loss: 0.44794, Test acc: 78.21%\n",
      "Epoch: 1590 | Loss: 0.46805, Accuracy: 81.46% | Test loss: 0.44790, Test acc: 78.21%\n",
      "Epoch: 1600 | Loss: 0.46801, Accuracy: 81.46% | Test loss: 0.44785, Test acc: 78.21%\n",
      "Epoch: 1610 | Loss: 0.46798, Accuracy: 81.46% | Test loss: 0.44781, Test acc: 78.21%\n",
      "Epoch: 1620 | Loss: 0.46794, Accuracy: 81.46% | Test loss: 0.44776, Test acc: 78.21%\n",
      "Epoch: 1630 | Loss: 0.46791, Accuracy: 81.46% | Test loss: 0.44772, Test acc: 78.21%\n",
      "Epoch: 1640 | Loss: 0.46787, Accuracy: 81.46% | Test loss: 0.44768, Test acc: 78.21%\n",
      "Epoch: 1650 | Loss: 0.46784, Accuracy: 81.46% | Test loss: 0.44763, Test acc: 78.21%\n",
      "Epoch: 1660 | Loss: 0.46780, Accuracy: 81.46% | Test loss: 0.44759, Test acc: 78.21%\n",
      "Epoch: 1670 | Loss: 0.46777, Accuracy: 81.46% | Test loss: 0.44754, Test acc: 78.21%\n",
      "Epoch: 1680 | Loss: 0.46773, Accuracy: 81.46% | Test loss: 0.44750, Test acc: 78.21%\n",
      "Epoch: 1690 | Loss: 0.46770, Accuracy: 81.46% | Test loss: 0.44746, Test acc: 78.21%\n",
      "Epoch: 1700 | Loss: 0.46766, Accuracy: 81.46% | Test loss: 0.44741, Test acc: 78.21%\n",
      "Epoch: 1710 | Loss: 0.46763, Accuracy: 81.46% | Test loss: 0.44737, Test acc: 78.21%\n",
      "Epoch: 1720 | Loss: 0.46760, Accuracy: 81.46% | Test loss: 0.44733, Test acc: 78.21%\n",
      "Epoch: 1730 | Loss: 0.46756, Accuracy: 81.46% | Test loss: 0.44729, Test acc: 78.21%\n",
      "Epoch: 1740 | Loss: 0.46753, Accuracy: 81.46% | Test loss: 0.44724, Test acc: 78.21%\n",
      "Epoch: 1750 | Loss: 0.46749, Accuracy: 81.46% | Test loss: 0.44720, Test acc: 78.21%\n",
      "Epoch: 1760 | Loss: 0.46746, Accuracy: 81.46% | Test loss: 0.44716, Test acc: 78.21%\n",
      "Epoch: 1770 | Loss: 0.46743, Accuracy: 81.46% | Test loss: 0.44712, Test acc: 78.21%\n",
      "Epoch: 1780 | Loss: 0.46739, Accuracy: 81.46% | Test loss: 0.44707, Test acc: 78.21%\n",
      "Epoch: 1790 | Loss: 0.46736, Accuracy: 81.46% | Test loss: 0.44703, Test acc: 78.21%\n",
      "Epoch: 1800 | Loss: 0.46732, Accuracy: 81.46% | Test loss: 0.44699, Test acc: 78.21%\n",
      "Epoch: 1810 | Loss: 0.46729, Accuracy: 81.46% | Test loss: 0.44695, Test acc: 78.21%\n",
      "Epoch: 1820 | Loss: 0.46726, Accuracy: 81.46% | Test loss: 0.44691, Test acc: 78.21%\n",
      "Epoch: 1830 | Loss: 0.46723, Accuracy: 81.46% | Test loss: 0.44687, Test acc: 78.21%\n",
      "Epoch: 1840 | Loss: 0.46719, Accuracy: 81.46% | Test loss: 0.44682, Test acc: 78.21%\n",
      "Epoch: 1850 | Loss: 0.46716, Accuracy: 81.46% | Test loss: 0.44678, Test acc: 78.21%\n",
      "Epoch: 1860 | Loss: 0.46713, Accuracy: 81.46% | Test loss: 0.44674, Test acc: 78.21%\n",
      "Epoch: 1870 | Loss: 0.46709, Accuracy: 81.46% | Test loss: 0.44670, Test acc: 78.21%\n",
      "Epoch: 1880 | Loss: 0.46706, Accuracy: 81.46% | Test loss: 0.44666, Test acc: 78.21%\n",
      "Epoch: 1890 | Loss: 0.46703, Accuracy: 81.46% | Test loss: 0.44662, Test acc: 78.21%\n",
      "Epoch: 1900 | Loss: 0.46700, Accuracy: 81.46% | Test loss: 0.44658, Test acc: 78.21%\n",
      "Epoch: 1910 | Loss: 0.46696, Accuracy: 81.32% | Test loss: 0.44654, Test acc: 78.21%\n",
      "Epoch: 1920 | Loss: 0.46693, Accuracy: 81.32% | Test loss: 0.44650, Test acc: 78.21%\n",
      "Epoch: 1930 | Loss: 0.46690, Accuracy: 81.32% | Test loss: 0.44645, Test acc: 78.21%\n",
      "Epoch: 1940 | Loss: 0.46687, Accuracy: 81.32% | Test loss: 0.44641, Test acc: 78.21%\n",
      "Epoch: 1950 | Loss: 0.46684, Accuracy: 81.32% | Test loss: 0.44637, Test acc: 78.21%\n",
      "Epoch: 1960 | Loss: 0.46680, Accuracy: 81.32% | Test loss: 0.44633, Test acc: 78.21%\n",
      "Epoch: 1970 | Loss: 0.46677, Accuracy: 81.32% | Test loss: 0.44629, Test acc: 78.21%\n",
      "Epoch: 1980 | Loss: 0.46674, Accuracy: 81.32% | Test loss: 0.44625, Test acc: 78.21%\n",
      "Epoch: 1990 | Loss: 0.46671, Accuracy: 81.32% | Test loss: 0.44621, Test acc: 78.21%\n",
      "Epoch: 2000 | Loss: 0.46668, Accuracy: 81.32% | Test loss: 0.44617, Test acc: 78.21%\n",
      "Epoch: 2010 | Loss: 0.46665, Accuracy: 81.32% | Test loss: 0.44613, Test acc: 78.21%\n",
      "Epoch: 2020 | Loss: 0.46661, Accuracy: 81.32% | Test loss: 0.44609, Test acc: 78.21%\n",
      "Epoch: 2030 | Loss: 0.46658, Accuracy: 81.32% | Test loss: 0.44605, Test acc: 78.21%\n",
      "Epoch: 2040 | Loss: 0.46655, Accuracy: 81.32% | Test loss: 0.44602, Test acc: 78.21%\n",
      "Epoch: 2050 | Loss: 0.46652, Accuracy: 81.32% | Test loss: 0.44598, Test acc: 78.21%\n",
      "Epoch: 2060 | Loss: 0.46649, Accuracy: 81.32% | Test loss: 0.44594, Test acc: 78.21%\n",
      "Epoch: 2070 | Loss: 0.46646, Accuracy: 81.32% | Test loss: 0.44590, Test acc: 78.21%\n",
      "Epoch: 2080 | Loss: 0.46643, Accuracy: 81.32% | Test loss: 0.44586, Test acc: 78.21%\n",
      "Epoch: 2090 | Loss: 0.46640, Accuracy: 81.32% | Test loss: 0.44582, Test acc: 78.21%\n",
      "Epoch: 2100 | Loss: 0.46637, Accuracy: 81.32% | Test loss: 0.44578, Test acc: 78.21%\n",
      "Epoch: 2110 | Loss: 0.46634, Accuracy: 81.32% | Test loss: 0.44574, Test acc: 78.21%\n",
      "Epoch: 2120 | Loss: 0.46631, Accuracy: 81.32% | Test loss: 0.44570, Test acc: 78.21%\n",
      "Epoch: 2130 | Loss: 0.46628, Accuracy: 81.32% | Test loss: 0.44567, Test acc: 78.21%\n",
      "Epoch: 2140 | Loss: 0.46625, Accuracy: 81.32% | Test loss: 0.44563, Test acc: 78.21%\n",
      "Epoch: 2150 | Loss: 0.46622, Accuracy: 81.32% | Test loss: 0.44559, Test acc: 78.21%\n",
      "Epoch: 2160 | Loss: 0.46618, Accuracy: 81.32% | Test loss: 0.44555, Test acc: 78.21%\n",
      "Epoch: 2170 | Loss: 0.46616, Accuracy: 81.32% | Test loss: 0.44551, Test acc: 78.21%\n",
      "Epoch: 2180 | Loss: 0.46613, Accuracy: 81.32% | Test loss: 0.44547, Test acc: 78.21%\n",
      "Epoch: 2190 | Loss: 0.46610, Accuracy: 81.32% | Test loss: 0.44544, Test acc: 78.21%\n",
      "Epoch: 2200 | Loss: 0.46607, Accuracy: 81.32% | Test loss: 0.44540, Test acc: 78.21%\n",
      "Epoch: 2210 | Loss: 0.46604, Accuracy: 81.32% | Test loss: 0.44536, Test acc: 78.21%\n",
      "Epoch: 2220 | Loss: 0.46601, Accuracy: 81.32% | Test loss: 0.44532, Test acc: 78.21%\n",
      "Epoch: 2230 | Loss: 0.46598, Accuracy: 81.32% | Test loss: 0.44528, Test acc: 78.21%\n",
      "Epoch: 2240 | Loss: 0.46595, Accuracy: 81.32% | Test loss: 0.44525, Test acc: 78.21%\n",
      "Epoch: 2250 | Loss: 0.46592, Accuracy: 81.32% | Test loss: 0.44521, Test acc: 78.21%\n",
      "Epoch: 2260 | Loss: 0.46589, Accuracy: 81.32% | Test loss: 0.44517, Test acc: 78.21%\n",
      "Epoch: 2270 | Loss: 0.46586, Accuracy: 81.32% | Test loss: 0.44514, Test acc: 78.21%\n",
      "Epoch: 2280 | Loss: 0.46583, Accuracy: 81.32% | Test loss: 0.44510, Test acc: 78.21%\n",
      "Epoch: 2290 | Loss: 0.46580, Accuracy: 81.18% | Test loss: 0.44506, Test acc: 78.21%\n",
      "Epoch: 2300 | Loss: 0.46577, Accuracy: 81.18% | Test loss: 0.44502, Test acc: 78.21%\n",
      "Epoch: 2310 | Loss: 0.46574, Accuracy: 81.18% | Test loss: 0.44499, Test acc: 78.21%\n",
      "Epoch: 2320 | Loss: 0.46572, Accuracy: 81.18% | Test loss: 0.44495, Test acc: 78.21%\n",
      "Epoch: 2330 | Loss: 0.46569, Accuracy: 81.18% | Test loss: 0.44491, Test acc: 78.21%\n",
      "Epoch: 2340 | Loss: 0.46566, Accuracy: 81.18% | Test loss: 0.44488, Test acc: 78.21%\n",
      "Epoch: 2350 | Loss: 0.46563, Accuracy: 81.18% | Test loss: 0.44484, Test acc: 78.21%\n",
      "Epoch: 2360 | Loss: 0.46560, Accuracy: 81.18% | Test loss: 0.44480, Test acc: 78.21%\n",
      "Epoch: 2370 | Loss: 0.46557, Accuracy: 81.18% | Test loss: 0.44477, Test acc: 78.21%\n",
      "Epoch: 2380 | Loss: 0.46555, Accuracy: 81.18% | Test loss: 0.44473, Test acc: 78.21%\n",
      "Epoch: 2390 | Loss: 0.46552, Accuracy: 81.18% | Test loss: 0.44470, Test acc: 78.21%\n",
      "Epoch: 2400 | Loss: 0.46549, Accuracy: 81.18% | Test loss: 0.44466, Test acc: 78.21%\n",
      "Epoch: 2410 | Loss: 0.46546, Accuracy: 81.18% | Test loss: 0.44462, Test acc: 78.21%\n",
      "Epoch: 2420 | Loss: 0.46543, Accuracy: 81.18% | Test loss: 0.44459, Test acc: 78.21%\n",
      "Epoch: 2430 | Loss: 0.46541, Accuracy: 81.18% | Test loss: 0.44455, Test acc: 78.21%\n",
      "Epoch: 2440 | Loss: 0.46538, Accuracy: 81.18% | Test loss: 0.44452, Test acc: 78.21%\n",
      "Epoch: 2450 | Loss: 0.46535, Accuracy: 81.18% | Test loss: 0.44448, Test acc: 78.21%\n",
      "Epoch: 2460 | Loss: 0.46532, Accuracy: 81.18% | Test loss: 0.44444, Test acc: 78.21%\n",
      "Epoch: 2470 | Loss: 0.46530, Accuracy: 81.18% | Test loss: 0.44441, Test acc: 78.21%\n",
      "Epoch: 2480 | Loss: 0.46527, Accuracy: 81.18% | Test loss: 0.44437, Test acc: 78.21%\n",
      "Epoch: 2490 | Loss: 0.46524, Accuracy: 81.18% | Test loss: 0.44434, Test acc: 78.21%\n",
      "Epoch: 2500 | Loss: 0.46521, Accuracy: 81.18% | Test loss: 0.44430, Test acc: 78.21%\n",
      "Epoch: 2510 | Loss: 0.46519, Accuracy: 81.18% | Test loss: 0.44427, Test acc: 78.21%\n",
      "Epoch: 2520 | Loss: 0.46516, Accuracy: 81.18% | Test loss: 0.44423, Test acc: 78.21%\n",
      "Epoch: 2530 | Loss: 0.46513, Accuracy: 81.18% | Test loss: 0.44420, Test acc: 78.21%\n",
      "Epoch: 2540 | Loss: 0.46510, Accuracy: 81.18% | Test loss: 0.44416, Test acc: 78.21%\n",
      "Epoch: 2550 | Loss: 0.46508, Accuracy: 81.18% | Test loss: 0.44413, Test acc: 78.21%\n",
      "Epoch: 2560 | Loss: 0.46505, Accuracy: 81.18% | Test loss: 0.44409, Test acc: 78.21%\n",
      "Epoch: 2570 | Loss: 0.46502, Accuracy: 81.18% | Test loss: 0.44406, Test acc: 78.21%\n",
      "Epoch: 2580 | Loss: 0.46500, Accuracy: 81.18% | Test loss: 0.44402, Test acc: 78.21%\n",
      "Epoch: 2590 | Loss: 0.46497, Accuracy: 81.18% | Test loss: 0.44399, Test acc: 78.21%\n",
      "Epoch: 2600 | Loss: 0.46494, Accuracy: 81.18% | Test loss: 0.44396, Test acc: 78.21%\n",
      "Epoch: 2610 | Loss: 0.46492, Accuracy: 81.18% | Test loss: 0.44392, Test acc: 78.21%\n",
      "Epoch: 2620 | Loss: 0.46489, Accuracy: 81.18% | Test loss: 0.44389, Test acc: 78.21%\n",
      "Epoch: 2630 | Loss: 0.46487, Accuracy: 81.18% | Test loss: 0.44385, Test acc: 78.21%\n",
      "Epoch: 2640 | Loss: 0.46484, Accuracy: 81.18% | Test loss: 0.44382, Test acc: 78.21%\n",
      "Epoch: 2650 | Loss: 0.46481, Accuracy: 81.18% | Test loss: 0.44378, Test acc: 78.21%\n",
      "Epoch: 2660 | Loss: 0.46479, Accuracy: 81.18% | Test loss: 0.44375, Test acc: 78.21%\n",
      "Epoch: 2670 | Loss: 0.46476, Accuracy: 81.18% | Test loss: 0.44372, Test acc: 78.21%\n",
      "Epoch: 2680 | Loss: 0.46473, Accuracy: 81.18% | Test loss: 0.44368, Test acc: 78.21%\n",
      "Epoch: 2690 | Loss: 0.46471, Accuracy: 81.18% | Test loss: 0.44365, Test acc: 78.21%\n",
      "Epoch: 2700 | Loss: 0.46468, Accuracy: 81.18% | Test loss: 0.44361, Test acc: 78.21%\n",
      "Epoch: 2710 | Loss: 0.46466, Accuracy: 81.18% | Test loss: 0.44358, Test acc: 78.21%\n",
      "Epoch: 2720 | Loss: 0.46463, Accuracy: 81.18% | Test loss: 0.44355, Test acc: 78.77%\n",
      "Epoch: 2730 | Loss: 0.46461, Accuracy: 81.18% | Test loss: 0.44351, Test acc: 78.77%\n",
      "Epoch: 2740 | Loss: 0.46458, Accuracy: 81.18% | Test loss: 0.44348, Test acc: 78.77%\n",
      "Epoch: 2750 | Loss: 0.46455, Accuracy: 81.18% | Test loss: 0.44345, Test acc: 78.77%\n",
      "Epoch: 2760 | Loss: 0.46453, Accuracy: 81.18% | Test loss: 0.44341, Test acc: 78.77%\n",
      "Epoch: 2770 | Loss: 0.46450, Accuracy: 81.18% | Test loss: 0.44338, Test acc: 78.77%\n",
      "Epoch: 2780 | Loss: 0.46448, Accuracy: 81.18% | Test loss: 0.44335, Test acc: 78.77%\n",
      "Epoch: 2790 | Loss: 0.46445, Accuracy: 81.18% | Test loss: 0.44332, Test acc: 78.77%\n",
      "Epoch: 2800 | Loss: 0.46443, Accuracy: 81.18% | Test loss: 0.44328, Test acc: 78.77%\n",
      "Epoch: 2810 | Loss: 0.46440, Accuracy: 81.18% | Test loss: 0.44325, Test acc: 78.77%\n",
      "Epoch: 2820 | Loss: 0.46438, Accuracy: 81.18% | Test loss: 0.44322, Test acc: 78.77%\n",
      "Epoch: 2830 | Loss: 0.46435, Accuracy: 81.18% | Test loss: 0.44318, Test acc: 78.77%\n",
      "Epoch: 2840 | Loss: 0.46433, Accuracy: 81.18% | Test loss: 0.44315, Test acc: 78.77%\n",
      "Epoch: 2850 | Loss: 0.46430, Accuracy: 81.18% | Test loss: 0.44312, Test acc: 78.77%\n",
      "Epoch: 2860 | Loss: 0.46428, Accuracy: 81.18% | Test loss: 0.44309, Test acc: 78.77%\n",
      "Epoch: 2870 | Loss: 0.46425, Accuracy: 81.18% | Test loss: 0.44305, Test acc: 78.77%\n",
      "Epoch: 2880 | Loss: 0.46423, Accuracy: 81.18% | Test loss: 0.44302, Test acc: 78.77%\n",
      "Epoch: 2890 | Loss: 0.46421, Accuracy: 81.18% | Test loss: 0.44299, Test acc: 78.77%\n",
      "Epoch: 2900 | Loss: 0.46418, Accuracy: 81.18% | Test loss: 0.44296, Test acc: 78.77%\n",
      "Epoch: 2910 | Loss: 0.46416, Accuracy: 81.18% | Test loss: 0.44292, Test acc: 78.77%\n",
      "Epoch: 2920 | Loss: 0.46413, Accuracy: 81.32% | Test loss: 0.44289, Test acc: 78.77%\n",
      "Epoch: 2930 | Loss: 0.46411, Accuracy: 81.32% | Test loss: 0.44286, Test acc: 78.77%\n",
      "Epoch: 2940 | Loss: 0.46408, Accuracy: 81.32% | Test loss: 0.44283, Test acc: 78.77%\n",
      "Epoch: 2950 | Loss: 0.46406, Accuracy: 81.32% | Test loss: 0.44280, Test acc: 78.77%\n",
      "Epoch: 2960 | Loss: 0.46404, Accuracy: 81.32% | Test loss: 0.44277, Test acc: 78.77%\n",
      "Epoch: 2970 | Loss: 0.46401, Accuracy: 81.32% | Test loss: 0.44273, Test acc: 78.77%\n",
      "Epoch: 2980 | Loss: 0.46399, Accuracy: 81.32% | Test loss: 0.44270, Test acc: 78.77%\n",
      "Epoch: 2990 | Loss: 0.46396, Accuracy: 81.32% | Test loss: 0.44267, Test acc: 78.77%\n",
      "Epoch: 3000 | Loss: 0.46394, Accuracy: 81.32% | Test loss: 0.44264, Test acc: 78.77%\n",
      "Epoch: 3010 | Loss: 0.46392, Accuracy: 81.32% | Test loss: 0.44261, Test acc: 78.21%\n",
      "Epoch: 3020 | Loss: 0.46389, Accuracy: 81.32% | Test loss: 0.44258, Test acc: 78.21%\n",
      "Epoch: 3030 | Loss: 0.46387, Accuracy: 81.32% | Test loss: 0.44254, Test acc: 78.21%\n",
      "Epoch: 3040 | Loss: 0.46384, Accuracy: 81.32% | Test loss: 0.44251, Test acc: 78.21%\n",
      "Epoch: 3050 | Loss: 0.46382, Accuracy: 81.18% | Test loss: 0.44248, Test acc: 78.21%\n",
      "Epoch: 3060 | Loss: 0.46380, Accuracy: 81.18% | Test loss: 0.44245, Test acc: 78.21%\n",
      "Epoch: 3070 | Loss: 0.46377, Accuracy: 81.18% | Test loss: 0.44242, Test acc: 78.21%\n",
      "Epoch: 3080 | Loss: 0.46375, Accuracy: 81.18% | Test loss: 0.44239, Test acc: 78.21%\n",
      "Epoch: 3090 | Loss: 0.46373, Accuracy: 81.18% | Test loss: 0.44236, Test acc: 78.21%\n",
      "Epoch: 3100 | Loss: 0.46370, Accuracy: 81.18% | Test loss: 0.44233, Test acc: 78.21%\n",
      "Epoch: 3110 | Loss: 0.46368, Accuracy: 81.18% | Test loss: 0.44230, Test acc: 78.21%\n",
      "Epoch: 3120 | Loss: 0.46366, Accuracy: 81.18% | Test loss: 0.44227, Test acc: 78.21%\n",
      "Epoch: 3130 | Loss: 0.46363, Accuracy: 81.18% | Test loss: 0.44223, Test acc: 78.21%\n",
      "Epoch: 3140 | Loss: 0.46361, Accuracy: 81.18% | Test loss: 0.44220, Test acc: 78.21%\n",
      "Epoch: 3150 | Loss: 0.46359, Accuracy: 81.18% | Test loss: 0.44217, Test acc: 78.21%\n",
      "Epoch: 3160 | Loss: 0.46357, Accuracy: 81.18% | Test loss: 0.44214, Test acc: 78.21%\n",
      "Epoch: 3170 | Loss: 0.46354, Accuracy: 81.18% | Test loss: 0.44211, Test acc: 78.21%\n",
      "Epoch: 3180 | Loss: 0.46352, Accuracy: 81.18% | Test loss: 0.44208, Test acc: 78.21%\n",
      "Epoch: 3190 | Loss: 0.46350, Accuracy: 81.18% | Test loss: 0.44205, Test acc: 78.21%\n",
      "Epoch: 3200 | Loss: 0.46348, Accuracy: 81.18% | Test loss: 0.44202, Test acc: 78.21%\n",
      "Epoch: 3210 | Loss: 0.46345, Accuracy: 81.18% | Test loss: 0.44199, Test acc: 78.21%\n",
      "Epoch: 3220 | Loss: 0.46343, Accuracy: 81.18% | Test loss: 0.44196, Test acc: 78.21%\n",
      "Epoch: 3230 | Loss: 0.46341, Accuracy: 81.18% | Test loss: 0.44193, Test acc: 78.21%\n",
      "Epoch: 3240 | Loss: 0.46339, Accuracy: 81.18% | Test loss: 0.44190, Test acc: 78.21%\n",
      "Epoch: 3250 | Loss: 0.46336, Accuracy: 81.18% | Test loss: 0.44187, Test acc: 78.21%\n",
      "Epoch: 3260 | Loss: 0.46334, Accuracy: 81.18% | Test loss: 0.44184, Test acc: 78.21%\n",
      "Epoch: 3270 | Loss: 0.46332, Accuracy: 81.18% | Test loss: 0.44181, Test acc: 78.21%\n",
      "Epoch: 3280 | Loss: 0.46330, Accuracy: 81.18% | Test loss: 0.44178, Test acc: 78.21%\n",
      "Epoch: 3290 | Loss: 0.46327, Accuracy: 81.18% | Test loss: 0.44175, Test acc: 78.21%\n",
      "Epoch: 3300 | Loss: 0.46325, Accuracy: 81.18% | Test loss: 0.44172, Test acc: 78.21%\n",
      "Epoch: 3310 | Loss: 0.46323, Accuracy: 81.18% | Test loss: 0.44169, Test acc: 78.21%\n",
      "Epoch: 3320 | Loss: 0.46321, Accuracy: 81.18% | Test loss: 0.44166, Test acc: 78.21%\n",
      "Epoch: 3330 | Loss: 0.46319, Accuracy: 81.18% | Test loss: 0.44163, Test acc: 78.21%\n",
      "Epoch: 3340 | Loss: 0.46316, Accuracy: 81.18% | Test loss: 0.44161, Test acc: 78.21%\n",
      "Epoch: 3350 | Loss: 0.46314, Accuracy: 81.18% | Test loss: 0.44158, Test acc: 78.21%\n",
      "Epoch: 3360 | Loss: 0.46312, Accuracy: 81.18% | Test loss: 0.44155, Test acc: 78.21%\n",
      "Epoch: 3370 | Loss: 0.46310, Accuracy: 81.18% | Test loss: 0.44152, Test acc: 78.21%\n",
      "Epoch: 3380 | Loss: 0.46308, Accuracy: 81.18% | Test loss: 0.44149, Test acc: 78.21%\n",
      "Epoch: 3390 | Loss: 0.46306, Accuracy: 81.18% | Test loss: 0.44146, Test acc: 78.21%\n",
      "Epoch: 3400 | Loss: 0.46303, Accuracy: 81.18% | Test loss: 0.44143, Test acc: 78.21%\n",
      "Epoch: 3410 | Loss: 0.46301, Accuracy: 81.18% | Test loss: 0.44140, Test acc: 78.21%\n",
      "Epoch: 3420 | Loss: 0.46299, Accuracy: 81.18% | Test loss: 0.44137, Test acc: 78.21%\n",
      "Epoch: 3430 | Loss: 0.46297, Accuracy: 81.18% | Test loss: 0.44134, Test acc: 78.21%\n",
      "Epoch: 3440 | Loss: 0.46295, Accuracy: 81.18% | Test loss: 0.44131, Test acc: 78.21%\n",
      "Epoch: 3450 | Loss: 0.46293, Accuracy: 81.18% | Test loss: 0.44129, Test acc: 78.21%\n",
      "Epoch: 3460 | Loss: 0.46291, Accuracy: 81.18% | Test loss: 0.44126, Test acc: 78.21%\n",
      "Epoch: 3470 | Loss: 0.46288, Accuracy: 81.18% | Test loss: 0.44123, Test acc: 78.21%\n",
      "Epoch: 3480 | Loss: 0.46286, Accuracy: 81.18% | Test loss: 0.44120, Test acc: 78.21%\n",
      "Epoch: 3490 | Loss: 0.46284, Accuracy: 81.18% | Test loss: 0.44117, Test acc: 78.21%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(88) # semilla aleatoria\n",
    "\n",
    "# Número de epocas\n",
    "epochs = 3500\n",
    "\n",
    "# Poner los datos en el hardware target\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Loop de training y eval\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_1.train()\n",
    "\n",
    "    # 1. Forward propagation (el modelo regresa logits)\n",
    "    y_logits = model_1(X_train).squeeze() # squeeze para remover `1` dimension extra\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> pred probs -> pred labls\n",
    "  \n",
    "    # 2. Se calcula loss/accuracy\n",
    "    # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n",
    "    #                y_train) \n",
    "    loss = loss_fn(y_logits, # nn.BCEWithLogitsLoss acepta los logits de salida\n",
    "                   y_train) \n",
    "    acc = accuracy_fn(y_true=y_train, \n",
    "                      y_pred=y_pred) \n",
    "\n",
    "    # 3. Zero grad para el optimizador\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Back propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Optimizador\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Evaluacion\n",
    "    model_1.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward \n",
    "        test_logits = model_1(X_test).squeeze() \n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "        # 2. loss/accuracy\n",
    "        test_loss = loss_fn(test_logits,\n",
    "                            y_test)\n",
    "        test_acc = accuracy_fn(y_true=y_test,\n",
    "                               y_pred=test_pred)\n",
    "\n",
    "    # Print cada 10 epocas\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
